# 선형 회귀 (Linear Regression) 심화 정리 노트

## 1. 회귀 (Regression) 소개

- **회귀**는 하나 이상의 독립 변수(feature)와 종속 변수(label) 간의 상관관계를 모델링하는 통계적 방법입니다.
- 데이터 포인트들 사이의 추세를 파악하여, 새로운 입력 값에 대한 연속적인 출력 값을 예측하는 데 사용됩니다.
- **예시**:
    - 주택 크기(독립 변수)에 따른 주택 가격(종속 변수) 예측
    - 공부 시간(독립 변수)에 따른 시험 성적(종속 변수) 예측

## 2. 선형 회귀 (Linear Regression)

- **선형 회귀**는 독립 변수와 종속 변수 사이에 **선형 관계**가 있다고 가정하고, 이 관계를 가장 잘 나타내는 직선(또는 고차원에서는 초평면)을 찾는 알고리즘입니다.
- 모델이 학습을 통해 데이터의 패턴을 가장 잘 나타내는 최적의 가중치(Weight)와 편향(bias)을 찾는 과정입니다.

### 2.1. 가설 함수 (Hypothesis Function)

- 선형 회귀의 가설 함수는 다음과 같은 1차 함수(직선의 방정식) 형태로 표현됩니다.

  `h(x) = Wx + b`

  - `h(x)`: 모델이 예측한 값 (predicted value)
  - `W` (Weight): 가중치, 직선의 기울기. 특성(x)이 예측값에 얼마나 큰 영향을 미치는지를 나타냅니다.
  - `b` (bias): 편향, y절편. 기본적인 출력값을 결정합니다.
  - `x`: 입력 특성 (input feature)

- **학습 목표**: 주어진 학습 데이터(x, y)를 이용하여, 예측값 `h(x)`와 실제값 `y`의 차이를 최소화하는 최적의 `W`와 `b`를 찾는 것입니다.

## 3. 비용 함수 (Cost Function)

- **비용 함수(Cost Function)** 또는 **손실 함수(Loss Function)**는 모델의 예측값(`h(x)`)과 실제값(`y`)의 차이, 즉 오차(error)를 측정하는 함수입니다.
- 이 함수의 값을 최소화하는 것이 모델 학습의 목표입니다.

### 3.1. 평균 제곱 오차 (Mean Squared Error, MSE)

- 선형 회귀에서는 비용 함수로 **평균 제곱 오차(MSE)**를 주로 사용합니다.

  `Cost(W, b) = (1/m) * Σ(h(x_i) - y_i)^2`

  - `m`: 전체 학습 데이터의 개수
  - `h(x_i)`: i번째 입력 데이터 `x_i`에 대한 모델의 예측값
  - `y_i`: i번째 실제 데이터의 값 (정답)
  - `Σ`: i=1부터 m까지의 모든 데이터에 대한 오차를 합산

### 3.2. 왜 평균 제곱 오차(MSE)를 사용하는가?

1.  **오차의 부호 처리**: 오차는 양수일 수도, 음수일 수도 있습니다. 단순 합산 시 오차가 상쇄될 수 있으므로, 제곱을 통해 모든 오차를 양수로 만듭니다.
2.  **패널티 부여**: 오차가 큰 값에 대해 제곱을 함으로써 더 큰 패널티를 부여하여, 모델이 큰 오차에 더 민감하게 반응하도록 합니다.
3.  **미분 용이성**: 비용 함수를 `W`와 `b`에 대해 미분하기 쉬운 형태가 되어, 경사 하강법을 적용하기에 유리합니다.

## 4. 경사 하강법 (Gradient Descent)

- **경사 하강법**은 비용 함수를 최소화하는 최적의 `W`와 `b`를 찾기 위한 대표적인 최적화(optimization) 알고리즘입니다.
- 비용 함수를 2차원 그래프로 그렸을 때, 가장 낮은 지점(최솟값)을 찾아가는 과정과 같습니다. 현재 위치에서 기울기(gradient)를 계산하고, 기울기가 가장 가파르게 감소하는 방향으로 `W`와 `b`를 점진적으로 업데이트합니다.

### 4.1. 알고리즘 과정

1.  `W`와 `b`를 임의의 값(보통 0 또는 무작위 값)으로 초기화합니다.
2.  현재 `W`와 `b`에서의 비용(Cost)을 계산합니다.
3.  비용 함수의 기울기(gradient), 즉 `W`와 `b`에 대한 편미분 값을 각각 계산합니다.
4.  계산된 기울기 방향으로 `W`와 `b`를 **학습률(learning rate)**만큼 이동(업데이트)시킵니다.
5.  비용 함수가 더 이상 감소하지 않거나(수렴), 지정된 횟수만큼 반복할 때까지 2-4번 과정을 반복합니다.

### 4.2. 업데이트 수식

  `W := W - α * (∂/∂W) * Cost(W, b)`
  `b := b - α * (∂/∂b) * Cost(W, b)`

  - `:=`: 값을 업데이트한다는 의미입니다.
  - `α` (alpha): **학습률(Learning Rate)**. `W`와 `b`를 업데이트할 보폭(step)을 결정합니다.
  - `(∂/∂W) * Cost(W, b)`: 비용 함수를 `W`에 대해 편미분한 값 (기울기). `W`가 변할 때 비용이 얼마나 변하는지를 나타냅니다.

### 4.3. 학습률 (Learning Rate, α)의 중요성

- **학습률이 너무 큰 경우**: 최적의 지점을 지나쳐 버려 값이 발산(divergence)할 수 있습니다. (overshooting)
- **학습률이 너무 작은 경우**: 학습 속도가 매우 느려지고, 전역 최솟값이 아닌 지역 최솟값(local minimum)에 빠질 위험이 있습니다.
- 따라서, 적절한 학습률을 찾는 것이 매우 중요합니다.

## 5. 다중 선형 회귀 (Multivariable Linear Regression)

- 주택 가격을 예측할 때, 주택 크기뿐만 아니라 방의 개수, 층수 등 여러 개의 독립 변수를 사용하여 종속 변수를 예측하는 경우입니다.

### 5.1. 가설 함수

  `h(x_1, x_2, ..., x_n) = w_1*x_1 + w_2*x_2 + ... + w_n*x_n + b`

- 이는 행렬(Matrix)의 곱셈을 사용하여 더 간결하게 표현할 수 있습니다.

  `h(X) = XW + b`

  - `X`: `m x n` 크기의 입력 특성 행렬 (m: 데이터 샘플 수, n: 특성 수)
  - `W`: `n x 1` 크기의 가중치 벡터
  - `b`: 편향

- 비용 함수와 경사 하강법 알고리즘은 단일 변수일 때와 동일한 원리로 적용됩니다.

### 5.2. 특성 스케일링 (Feature Scaling)

- 여러 특성의 단위나 값의 범위가 크게 다를 경우(예: 방의 개수(1~10) vs 주택 가격(수억 원)), 비용 함수의 표면이 찌그러진 형태가 되어 경사 하강법이 최적의 값을 찾는 데 오랜 시간이 걸릴 수 있습니다.
- **표준화(Standardization)**나 **정규화(Normalization)**와 같은 기법을 통해 특성들의 스케일을 비슷하게 맞춰주면, 경사 하강법이 더 빠르고 안정적으로 수렴하도록 도울 수 있습니다.

## 6. 선형 회귀의 기본 가정

선형 회귀 모델이 좋은 성능을 내기 위해 만족해야 하는 4가지 기본 가정이 있습니다.

1.  **선형성(Linearity)**: 독립 변수와 종속 변수 간에 선형적인 관계가 존재해야 합니다.
2.  **독립성(Independence)**: 오차(residuals)들은 서로 독립적이어야 합니다.
3.  **등분산성(Homoscedasticity)**: 오차의 분산은 모든 독립 변수 값에 대해 일정해야 합니다.
4.  **정규성(Normality)**: 오차는 정규분포를 따라야 합니다.

이 가정들이 깨지면 모델의 예측 성능이 저하될 수 있습니다.