Machine Learning

ë™ì•„ëŒ€í•™êµ ì»´í“¨í„°ê³µí•™ê³¼/AIí•™ê³¼ í•œì •ê·œ

1

Linear Regression

2

Contents
ï‚– What is Regression?
ï‚– Simple Linear Regression

ï‚– Training : Gradient Descent
ï‚– Usage 1: Prediction
ï‚– Apply Linear Regression on Real Dataset

ï‚– Usage 2: Correlation Analysis

3

What is Regression?

4

Regression

Weight (kg)

ë¬¸ì œ: ì•„ë˜ ê·¸ë˜í”„ë¥¼ í† ëŒ€ë¡œ ì–´ëŠ ì‚¬ëŒì˜ í‚¤ê°€ ì£¼ì–´ì¡Œì„ ë•Œ,
ìš°ë¦¬ëŠ” ê·¸ ì‚¬ëŒì˜ ëª¸ë¬´ê²Œë¥¼ ì–´ëŠì •ë„ëŠ” ì˜ˆì¸¡í•  ìˆ˜ ìˆì„ê¹Œ?
120
110
100
90
80
70
60
50
40
30
20
10
0

150

160

170

180

Height (cm)

190

200

5

Regression (Contâ€™d)

Weight (kg)

ë§Œì•½ ì˜ˆì¸¡í•  ìˆ˜ ìˆë‹¤ë©´ ì–´ë–»ê²Œ?
120
110
100
90
80
70
60
50
40
30
20
10
0

ê²½í–¥ (Trend)

150

160

170

180

Height (cm)

190

200

6

What is Regression? : From usersâ€™ perspective.
ï‚– ì£¼ì–´ì§„ ë°ì´í„° ì¸ìŠ¤í„´ìŠ¤(Data Instance)ë¥¼ ë‚˜íƒ€ë‚´ëŠ”, ì´ë¯¸ ì•Œê³  ìˆëŠ” í•˜ë‚˜ í˜¹ì€ ì—¬ëŸ¬ ê°œì˜ íŠ¹
ì§•(Feature, í˜¹ì€ independent variables)ìœ¼ë¡œë¶€í„° ëª©í‘œí•œ íŠ¹ì§•(target variable, dependent
variable)ì„ ì˜ˆì¸¡í•˜ëŠ” ê²ƒ.
ï‚– ì˜ˆ
ï‚– ì£¼ì–´ì§„ ë°ì´í„° ì¸ìŠ¤í„´ìŠ¤: ê°œì¸ (ê°œë³„ì ì¸ ì‚¬ëŒ).
ï‚– ì´ë¯¸ ì•Œê³  ìˆëŠ” í•˜ë‚˜ í˜¹ì€ ì—¬ëŸ¬ ê°œì˜ íŠ¹ì§• : í‚¤, ìš´ë™ëŸ‰, ì‹ì‚¬ ì¹¼ë¡œë¦¬ ë“±.
ï‚– ì˜ˆì¸¡í•˜ëŠ” íŠ¹ì§• : ëª¸ë¬´ê²Œ
ï‚– ì˜ˆì¸¡ë˜ëŠ” íŠ¹ì§•ì´ ê°€ì§ˆ ìˆ˜ ìˆëŠ” ê°’ì€ ì—°ì†ëœ ì‹¤ìˆ˜(Real number)ë²”ìœ„ì˜ ê°’ì´ë‹¤.
ï‚– ê°’ì˜ ê±°ë¦¬ ì°¨ì´ê°€ ì˜ë¯¸ê°€ ìˆìŒ
ï‚– ì˜ˆ) ëª¸ë¬´ê²ŒëŠ” 60, 78.9, 109.3 ë“± ì‹¤ìˆ˜ ê°’ìœ¼ë¡œ í‘œí˜„ëœë‹¤.
ï‚– Categorical value(ë²”ì£¼í˜• ê°’)ê°€ ì•„ë‹˜
ï‚– Categorical value: ê°’ì´ í•˜ë‚˜ì˜ ì¹´í…Œê³ ë¦¬ë¥¼ ë‚˜íƒ€ë‚´ëŠ” ê°’.
ï‚– ì˜ˆ 1) ì„±ë³„ í‘œí˜„: ë‚¨ì -> 1, ì—¬ì -> 2
ï‚– ì˜ˆ 2) ì˜í™” ì¥ë¥´: ì•¡ì…˜ -> 1, ë¡œë§¨ìŠ¤ -> 2, ìŠ¤ë¦´ëŸ¬ -> 3, SF -> 4

7

What is regression? : More formal definition
In statistical modeling, regression analysis is a set of statistical processes for
estimating the relationships between a dependent variable (Output variable: often called the
'outcome' or 'response' variable) and one or more independent variables (Input variables: often
called 'predictors', 'covariates', 'explanatory variables' or 'featuresâ€™).
(Excerpted from Wikipedia)

8

Relationships between variables

Weight (kg)

ë‹¤ìŒ ê·¸ë¦¼ì—ì„œ ë³€ìˆ˜ Heightì™€ WeightëŠ” ì–´ë–¤ ê´€ê³„ì„±ì´ ìˆì–´ ë³´ì¸ë‹¤?
Heightê°€ ì£¼ì–´ì§€ë©´ Weightë¥¼ ì–´ëŠì •ë„ ì˜ˆì¸¡í•´ ë³¼ ìˆ˜ ìˆëŠ”ê°€?
120
110
100
90
80
70
60
50
40
30
20
10
0

150

160

170

180

190

200

Height (cm)

9

Relationships between variables (Contâ€™d)
ë‹¤ìŒ ê·¸ë¦¼ì—ì„œ ë³€ìˆ˜ Heightì™€ WeightëŠ” ì–´ë–¤ ê´€ê³„ì„±ì´ ìˆì–´ ë³´ì¸ë‹¤?
x ê°’ì´ ì£¼ì–´ì§€ë©´ yê°’ì„ ì–´ëŠì •ë„ ì˜ˆì¸¡í•´ ë³¼ ìˆ˜ ìˆëŠ”ê°€?
1.00
0.80
0.60
0.40

0.20
0.00
0.00

0.20

0.40

0.60

0.80

1.00

10

Relationships between variables (Contâ€™d)

Randolph, Justus. (2007). Multidisciplinary methods in
educational technology research and development.

https://www.statisticshowto.com/quadra
tic-regression/

11

Linear Relation
Linear Relation : ë³€ìˆ˜ ì‚¬ì´ì˜ ê´€ê³„ê°€ ì§ì„  (n ì°¨ì› ê³µê°„ì—ì„œëŠ” hyper-planeìœ¼ë¡œ í‘œí˜„ë˜ëŠ” ê´€ê³„)

Positive Linear Relation between x and y.

Negative Linear Relation between x and y.

12

What is regression? : More formal definition (Again)
In statistical modeling, regression analysis is a set of statistical processes for
estimating the relationships between a dependent variable (Output variable: often called the
'outcome' or 'response' variable) and one or more independent variables (Input variables: often
called 'predictors', 'covariates', 'explanatory variables' or 'featuresâ€™).
(Excerpted from Wikipedia)

13

Linear Regression
ï‚– Linear Regressionì´ë€

ï‚– Outcome variable ì„ í•˜ë‚˜ ì´ìƒì˜ feature variableë“¤ì˜ ì„ í˜• ê²°í•©ìœ¼ë¡œ ì˜ˆì¸¡í•˜ëŠ” Regression.

ğ‘¦ = Ïƒğ‘˜ğ‘–=1 ğ‘¤ğ‘– ğ‘¥ğ‘– + ğ‘
ğ‘¦ : Outcome variable (ì˜ˆì¸¡í•˜ë ¤ëŠ” ë³€ìˆ˜).
ğ’™ğŸ , ğ’™ğŸ ,â€¦, ğ’™ğ’Œ : Feature variables. (ì´í•˜ ì¤„ì—¬ì„œ featureë¼ í•œë‹¤.)
ğ’˜ğŸ , ğ’˜ğŸ ,â€¦, ğ’˜ğ’Œ : Weights for each feature variable.
ğ‘ : bias

Feature 3ê°œì¸ Linear Regression ì˜ˆ)

y = 1.5 * ğ’™ğŸ - 0.1* ğ’™ğŸ + 0.5 ğ’™ğŸ‘ + b
y : ëª¸ë¬´ê²Œ
ğ’™ğŸ : í‰ê·  ì‹ì‚¬ëŸ‰
ğ’™ğŸ : ìš´ë™ëŸ‰
ğ’™ğŸ‘ : ë³µë¶€ ë‘˜ë ˆ

14

Why linear regression?
ï‚– Imagine we want to predict the outcome variable y by function ğ’‡ of k input feat
ures such as ğ’™ğŸ , ğ’™ğŸ ,â€¦, ğ’™ğ’Œ .
ï‚– ğ‘¦ = ğ‘“(ğ’™ğŸ , ğ’™ğŸ ,â€¦, ğ’™ğ’Œ )

ï‚– However, in most cases, we do not have enough data to directly estimate ğ’‡.
ï‚– Therefore, we usually have to assume that it has some restricted form (Assumpti
on, or model), such as linear.
ï‚– ğ‘¦ = Ïƒğ‘˜ğ‘–=1 ğ‘¤ğ‘– ğ‘¥ğ‘– + ğ‘ = ğ’˜ âˆ™ ğ’™ + ğ’ƒ
ï‚– ğ’˜ =<ğ’˜ğŸ , ğ’˜ğŸ ,â€¦, ğ’˜ğ’Œ >
ï‚– ğ’™ =<ğ’™ğŸ , ğ’™ğŸ ,â€¦, ğ’™ğ’Œ >

15

Simple Linear Regression
ï‚– Feature ê°€ í•˜ë‚˜ë°–ì— ì—†ëŠ” ë‹¤ìŒ Linear regressionì„ ìƒê°í•´ ë³´ì.
ï‚– ğ‘¦ = Ïƒ1ğ‘–=1 ğ‘¤ğ‘– ğ‘¥ğ‘– + ğ‘
ï‚– <=> ğ‘¦ = ğ‘¤1 ğ‘¥1 + ğ‘
ï‚– <=> ğ‘¦ = ğ‘¤ğ‘¥ + ğ‘ (í•˜ë‚˜ ë°–ì— ì—†ìœ¼ë¯€ë¡œ w, xì—ì„œ subscript 1ì„ ì‚­ì œ)
ï‚– ì¦‰, ìœ„ì™€ ê°™ì€ í˜•íƒœì˜ ë‹¨ìˆœí•œ Linear Regressionì€ í•˜ë‚˜ì˜ featureì™€ ì˜ˆì¸¡í•˜ë ¤ëŠ” outcome variableì™€ì˜ ì„ 
í˜• ê´€ê³„(Linear Relationship)ë¥¼ í•™ìŠµí•˜ëŠ” ê²ƒ

Weight (kg)

ï‚– ì˜ˆ) Height (Feature)ì™€ Weight (Outcome variable)ì˜ ê´€ê³„ì„±
120
110
100
90
80
70
60
50
40
30
20
10
0

ê´€ê³„ì„±

150

160

170

180

Height (cm)

190

200

16

Training :
Gradient Descent
17

Simple Linear Regression
ï‚– Simple Linear regressionì—ì„œ ğ‘¦ ì™€ ğ‘¥ ê³¼ì˜ ê´€ê³„ëŠ” ì•„ë˜ ì‹ìœ¼ë¡œ í‘œí˜„ëœë‹¤.
ï‚– ğ‘¦ = ğ‘¤ğ‘¥ + ğ‘

ï‚– ì„ í˜• ê´€ê³„(Linear Relationship)ë¥¼ í•™ìŠµí•˜ëŠ” ê²ƒì€ ë‘ ë³€ìˆ˜(ğ‘¦ ì™€ ğ‘¥) ì‚¬ì´ì˜ ê´€ê³„ì„±ì„ ê°€
ì¥ ì˜ í‘œí˜„í•˜ëŠ” ì§ì„ ì„ ì°¾ëŠ” ê²ƒì´ ëœë‹¤.
ï‚– ë‹¤ë¥´ê²Œ ë§í•˜ë©´ ìœ„ ì‹ì—ì„œ ğ‘¤, ğ‘ ì˜ ê°’ì„ ì°¾ëŠ” ê²ƒì´ ëœë‹¤.

Weight (kg)

ï‚– ê·¸ë ‡ë‹¤ë©´ ì–´ë–¤ ì§ì„ ì´ ê´€ê³„ì„±ì„ ê°€ì¥ ì˜ í‘œí˜„í•˜ëŠ” ì§ì„ ì¸ê°€?
Line 2

120
110
100
90
80
70
60
50
40
30
20
10
0

Line 1

Line 3
150

160

170

180

Height (cm)

190

200

18

How to fit (Train)? : Minimum Error
ê°ê°ì˜ Training dataì™€ ì˜ˆì¸¡í•œ ì„  ì‚¬ì´ì˜ ê±°ë¦¬ì˜ í•©ì´ ì ìœ¼ë©´ ì ì„ìˆ˜ë¡ ì¢‹ë‹¤.

Good fit

Bad fit

19

Objective Function & Model Parameter
ï‚– Objective Function (ëª©ì í•¨ìˆ˜) : ëª¨ë¸í•™ìŠµì„ ìœ„í•œ ëª©í‘œ(ë°©í–¥, ê°€ì´ë“œë¼ì¸)ì„ ì œ
ì‹œí•˜ëŠ” í•¨ìˆ˜.
ï‚– ì¼ë°˜ì ìœ¼ë¡œ ëª¨ë¸ í•™ìŠµì€ ëª©ì í•¨ìˆ˜ì˜ ê°’ì„ Minimization(ìµœì†Œí™”) í˜¹ì€ Maximization(ìµœëŒ€
í™”)í•˜ëŠ” Model parameter ê°’ì„ í•™ìŠµí•˜ëŠ” ê²ƒì´ë‹¤.
ï‚– í•™ìŠµ ì˜ ëœ ëª¨ë¸ì´ë€ ëª©ì í•¨ìˆ˜ì˜ ìµœì†Œê°’ í˜¹ì€ ìµœëŒ€ê°’ì— ìµœëŒ€í•œ ê·¼ì ‘í•œ parameterë¥¼ í•™ìŠµ
í•œ ëª¨ë¸ì´ë‹¤.

ï‚– Model parameter
ï‚– ì‹ìœ¼ë¡œ ë‚˜íƒ€ë‚¸ ëª¨ë¸ì—ì„œ ì…ë ¥ê³¼ ì¶œë ¥ ì´ì™¸ì˜, ê·¸ ê°’ì„ í•™ìŠµí•´ì•¼ í•  ë³€ìˆ˜.
ï‚– ëª¨ë¸ì˜ ì‹¤ì œ í˜•íƒœ(instance)ë¥¼ ê²°ì •í•˜ëŠ” parameter
ï‚– Programmingì˜ Class ë° instanceì˜ ê´€ê³„ì™€ ìœ ì‚¬
ï‚– Line => class, ì‹¤ì œ ê·¸ë ¤ì§„ ì§ì„  => instance
ï‚– ì˜ˆ) ë‹¤ìŒ simple linear regression ì‹ ğ‘¦ = ğ‘¤ğ‘¥ + ğ‘ ì—ì„œ í•™ìŠµí•´ì•¼ í•  Model parameterëŠ”?

20

Objective Function : MSE (Mean Squared Error)
ï‚– MSE:
ï‚– ê°ê°ì˜ Training dataì— ëŒ€í•´, â€œëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ ì˜ˆì¸¡í•œ ê°’â€ê³¼ â€œì‹¤ì œ ê°’â€ì˜ ì°¨ì´ì˜ ì œê³±ì˜ í•©.
Training data ê°€ < ğ‘¥1 , ğ‘¦1 >, < ğ‘¥2 , ğ‘¦2 > â€¦, < ğ‘¥ğ‘› , ğ‘¦ğ‘› > ì˜ nìŒ ìˆì„ ë•Œ, Simple linear
regression â€œğ‘¦ = ğ‘¤ğ‘¥ + ğ‘â€ ì˜ MSEëŠ” ë‹¤ìŒê³¼ ê°™ë‹¤.
1
ğ‘›
ğ‘›

MSE = Ïƒğ‘›ğ‘–=1 ğ‘¦ğ‘– âˆ’ ğ‘¦à·œğ‘– 2
1
= à· ğ‘¦ğ‘– âˆ’ (ğ‘¤ âˆ™ ğ‘¥ğ‘– + ğ‘) 2
ğ‘›
ğ‘–=1

ğ’šğ’Š : ğ‘– ë²ˆì§¸ ë°ì´í„°ì— ëŒ€í•œ ì •ë‹µê°’.
ğ’šà·ğ’Š : ğ‘– ë²ˆì§¸ ë°ì´í„°ì— ëŒ€í•œ ì˜ˆì¸¡ê°’.
ğ’: dataì˜ ê°œìˆ˜
ğ’™ğ’Š : ğ‘– ë²ˆì§¸ ë°ì´í„°ì— ëŒ€í•œ ì…ë ¥ê°’.

21

Training
ï‚– MSEì˜ ê°’ì„ ìµœì†Œí™”í•˜ê¸° ìœ„í•œ ğ‘¤ê³¼ ğ‘ì˜ ê°’ì€ ì–´ë–»ê²Œ êµ¬í•˜ëŠ”ê°€?
1
ğ‘›

MSE = Ïƒğ‘›ğ‘–=1 ğ‘¦ğ‘– âˆ’ (ğ‘¤ âˆ™ ğ‘¥ğ‘– + ğ‘) 2
ë¥¼ ì˜ ì‚´í´ë³´ë©´, MSEëŠ” ğ‘¤, ğ‘ ë¥¼ ì…ë ¥ ë³€ìˆ˜ë¡œ í•˜ëŠ” í•¨ìˆ˜ë¼ ë³¼ ìˆ˜ ìˆìŒ
1

MSE(ğ‘¤, ğ‘) = ğ‘› Ïƒğ‘›ğ‘–=1 ğ‘¦ğ‘– âˆ’ (ğ‘¤ âˆ™ ğ‘¥ğ‘– + ğ‘) 2
ê·¸ë ‡ë‹¤ë©´ ğ‘›, ğ‘¦ğ‘– , ğ‘¥ğ‘– ëŠ”? ë³€ìˆ˜?
ì§ì ‘ í•´ë³´ê¸°: ğ‘› = 2ë¼ê³  í•  ê²½ìš° ìœ„ MSEì˜ ì‹ì„ ì „ê°œí•˜ì—¬ ğ‘¤ì— ëŒ€í•´ ì •ë¦¬í•˜ë¼.

22

Training (Contâ€™d)
ï‚– ğ‘™ğ‘– = ğ‘¥ğ‘–2 âˆ™ ğ‘¤ 2 + 2(ğ‘¥ğ‘– âˆ™ ğ‘ âˆ’ ğ‘¥ğ‘– âˆ™ ğ‘¦ğ‘– ) âˆ™ ğ‘¤+ ğ‘¦ğ‘–2 âˆ’ 2 âˆ™ ğ‘ âˆ™ ğ‘¦ğ‘– + ğ‘ 2

ï‚– ì—ì„œ MSEë¥¼ ğ‘¤ ì— ê´€í•œ í•¨ìˆ˜ë¡œ ë³´ë©´ MSEëŠ” ğ’˜ì˜ 2ì°¨ í•¨ìˆ˜ë‹¤.
ï‚– ë˜ëŠ” ğ’ƒ ì˜ 2ì°¨í•¨ìˆ˜ì´ë‹¤.

23

2ì°¨ í•¨ìˆ˜ (Convex function or Concave Function)ì˜ íŠ¹ì§•.
y(MSE)

2ì°¨ í•¨ìˆ˜ lì´ ê°€ì§ˆ ìˆ˜ ìˆëŠ” ìµœëŒ€, í˜¹ì€ ìµœì†Œì ì€ í•˜ë‚˜ì´ë‹¤.
í˜¹ì€ ìµœì†Œì ì—ì„œì˜ Gradient(ë¯¸ë¶„ê°’)ì€ 0ì´ë‹¤.

ğ‘¥(ğ‘¤)

Gradientê°€ 0ì¸ ì ì€ ìµœì†Œ(Convex Functionì¼ ê²½ìš°) í˜¹ì€
ìµœëŒ€(Concave Functionì¼ ê²½ìš°) ì  ë¿ì´ë‹¤.

Convex Functionì˜ ìµœì†Œì : ê¸°ìš¸ê¸° 0.

24

2ì°¨ í•¨ìˆ˜ (Convex function or Concave Function)ì˜ íŠ¹ì§• (Contâ€™d)
y(MSE)
Convex Functionì—ì„œ
ìµœì†Œì ë³´ë‹¤ x (parameter)ê°€ í° ì ì€ Gradientê°€ í•­ìƒ +ì´ë‹¤.
ìµœì†Œì ë³´ë‹¤ x (parameter)ê°€ ì‘ì€ ì ì€ Gradientê°€ í•­ìƒ â€“ì´ë‹¤.

ğ‘¥(ğ‘¤) (Concave Functionì—ì„œ
ìµœëŒ€ì ë³´ë‹¤ x (parameter)ê°€ í° ì ì€ Gradientê°€ í•­ìƒ â€“ì´ë‹¤.
ìµœëŒ€ì ë³´ë‹¤ x (parameter)ê°€ ì‘ì€ ì ì€ Gradientê°€ í•­ìƒ +ì´ë‹¤.
)

Convex Functionì˜ ìµœì†Œì : ê¸°ìš¸ê¸° 0.

25

Stochastic Gradient Descent (Contâ€™d)
y(MSE)

Convex Functionì—ì„œ
ìµœì†Œì ë³´ë‹¤ x (parameter)ê°€ í° ì ì€ Gradientê°€ í•­ìƒ +ì´ë‹¤.
ìµœì†Œì ë³´ë‹¤ x (parameter)ê°€ ì‘ì€ ì ì€ Gradientê°€ í•­ìƒ â€“ì´ë‹¤.

ğ‘¥(ğ‘¤)

ë”°ë¼ì„œ í˜„ì¬ x ê°’ì„ ë°”íƒ•ìœ¼ë¡œ í•¨ìˆ˜ì˜ ìµœì†Œì ì„ ì°¾ê³  ì‹¶ë‹¤ë©´,
í˜„ì¬ì˜ Gradientì˜ ë°˜ëŒ€ ë°©í–¥ìœ¼ë¡œ x ê°’ì„ ì›€ì§ì—¬ì„œ (update)í•´ì„œ
Gradientê°€ 0ì¸ x ê°’ì„ ì°¾ì•„ ë‚˜ê°„ë‹¤.

x=2 ì—ì„œ ìµœì†Œ ì ì„ ì°¾ê¸° ìœ„í•´ì„œëŠ” x=2 ë¥¼
Gradientì˜ ë°˜ëŒ€ë°©í–¥ (ìŒì˜ ë°©í–¥)ìœ¼ë¡œ
ì—…ë°ì´íŠ¸.

Convex Functionì˜ ìµœì†Œì : ê¸°ìš¸ê¸° 0.
x=-1 ì—ì„œ ìµœì†Œ ì ì„ ì°¾ê¸° ìœ„í•´ì„œëŠ” x=-1
ë¥¼ Gradientì˜ ë°˜ëŒ€ë°©í–¥ (ì–‘ì˜ ë°©í–¥)ìœ¼ë¡œ
ì—…ë°ì´íŠ¸.

26

Stochastic Gradient Descent (Contâ€™d)
Concave Functionì—ì„œ
ìµœëŒ€ì ë³´ë‹¤ x (parameter)ê°€ í° ì ì€ Gradientê°€ í•­ìƒ -ì´ë‹¤.
ìµœëŒ€ì ë³´ë‹¤ x (parameter)ê°€ ì‘ì€ ì ì€ Gradientê°€ í•­ìƒ +ì´ë‹¤.

ë”°ë¼ì„œ í˜„ì¬ x ê°’ì„ ë°”íƒ•ìœ¼ë¡œ í•¨ìˆ˜ì˜ ìµœì†Œì ì„ ì°¾ê³  ì‹¶ë‹¤ë©´,
í˜„ì¬ì˜ Gradientì˜ ë°©í–¥ìœ¼ë¡œ x ê°’ì„ ì›€ì§ì—¬ì„œ (update)í•´ì„œ
Gradientê°€ 0ì¸ x ê°’ì„ ì°¾ì•„ ë‚˜ê°„ë‹¤.
x=1 ì—ì„œ ìµœëŒ€ì ì„ ì°¾ê¸° ìœ„í•´ì„œëŠ” x=1 ë¥¼
Gradientì˜ ë°©í–¥ (ìŒì˜ ë°©í–¥)ìœ¼ë¡œ
ì—…ë°ì´íŠ¸.

Concave Functionì˜ ìµœëŒ€ì : ê¸°ìš¸ê¸° 0.
x=-1 ì—ì„œ ìµœì†Œ ì ì„ ì°¾ê¸° ìœ„í•´ì„œëŠ” x=-1
ë¥¼ Gradientì˜ ë°©í–¥ (ì–‘ì˜ ë°©í–¥)ìœ¼ë¡œ
ì—…ë°ì´íŠ¸.

27

Gradient Descent ì•Œê³ ë¦¬ì¦˜
Objective Function fì˜ ê°’ì„ ìµœì†Œí™”í•˜ëŠ” ì…ë ¥ê°’ ğ‘¤ì— ëŒ€í•œ ìµœì ê°’ (Optimal value) ğ‘¤ âˆ— ì€,
ìµœì‹  wê°’ì— ë‹¤ìŒ updateë¥¼ ë°˜ë³µí•˜ì—¬ ê³„ì‚°í•  ìˆ˜ ìˆìŒ

ğœ•ğ‘“
ğ’˜â†ğ’˜ âˆ’ğ›‚
ğœ•ğ‘¤
ğœ¶: learning rate
1

MSE(ğ‘¤, ğ‘) = ğ‘› Ïƒğ‘›ğ‘–=1 ğ‘¦ğ‘– âˆ’ (ğ‘¤ âˆ™ ğ‘¥ğ‘– + ğ‘) 2 ì— ëŒ€í•œ ğ’˜ì˜ updateëŠ” ë‹¤ìŒê³¼ ê°™ì´ ì“¸ ìˆ˜ ìˆìŒ

ğœ•MSE(ğ‘¤, ğ‘)
ğ’˜â†ğ’˜ âˆ’ğ›‚
ğœ•ğ‘¤
ğœ•MSE(ğ‘¤,ğ‘)
=
ğœ•ğ‘¤

1
ğ‘›

ğœ• Ïƒğ‘›
ğ‘–=1 ğ‘¦ğ‘– âˆ’ ğ‘¤âˆ™ğ‘¥ğ‘– +ğ‘
ğœ•ğ‘¤

2

ğ ğ’š âˆ’ ğ’˜âˆ™ğ’™ğ’Š +ğ’ƒ
1
= Ïƒğ‘›ğ‘–=1 ğ’Š
ğ‘›
ğğ’˜

ğŸ

(by ë¯¸ë¶„ì˜ í•©ê·œì¹™)

28

Gradient Descent Algorithm for Linear Regression
Input : Training Data D: (< ğ‘¥1 , ğ‘¦1 >, < ğ‘¥2 , ğ‘¦2 > â€¦, < ğ‘¥ğ‘› , ğ‘¦ğ‘› >),
Objective Function: ğ‘“, learning rate: ğ›‚, max_iterations: ğ’Šğ’ğ’‚ğ’™ , epsilon : ğœ€
Output : ğ‘¤, b.
{
Randomly initialize the elements of ğ‘¤ and b.
ğ‘™ğ‘œğ‘™ğ‘‘ = 0
for 1 to ğ’Šğ’ğ’‚ğ’™ :
ğœ•ğ‘“

ğ’˜ â† ğ’˜ âˆ’ ğ›‚ ğœ•ğ‘¤
ğœ•ğ‘“

ğ’ƒ â† ğ’ƒ âˆ’ ğ›‚ ğœ•ğ‘
calculate ğ‘™ = Ïƒğ‘›ğ‘–=0 ğ‘¦ğ‘– âˆ’ (ğ‘¤ âˆ™ ğ‘¥ğ‘– + ğ‘) 2
if |ğ‘™ - ğ‘™ğ‘œğ‘™ğ‘‘ | < ğœ€ :
break;
else
ğ‘™ğ‘œğ‘™ğ‘‘ = ğ‘™
endfor
Return ğ’˜, ğ’ƒ.
}

29

Gradient Descent Algorithmì˜ ë¬¸ì œ
ï‚– ê³„ì‚° ì†ë„ê°€ ëŠë¦¬ë‹¤.
ï‚– ì™œ?

30

Stochastic Gradient Descent
ğœ•MSE(ğ‘¤,ğ‘)
=
ğœ•ğ‘¤

1
ğ‘›

ğœ• Ïƒğ‘›
ğ‘–=1 ğ‘¦ğ‘– âˆ’ ğ‘¤âˆ™ğ‘¥ğ‘– +ğ‘

ğ’šğ’Š âˆ’ ğ’˜ âˆ™ ğ’™ğ’Š + ğ’ƒ

ğœ•ğ‘¤
ğŸ

2

ğ ğ’š âˆ’ ğ’˜âˆ™ğ’™ğ’Š +ğ’ƒ
1
= Ïƒğ‘›ğ‘–=1 ğ’Š
ğ‘›
ğğ’˜

ğŸ

(by ë¯¸ë¶„ì˜ í•©ê·œì¹™)

= ğ’ğ’Š ë¼ í‘œí˜„í•˜ë©´

31

Stochastic Gradient Descent for linear regression
Input : Training Data D: (< ğ‘¥1 , ğ‘¦1 >, < ğ‘¥2 , ğ‘¦2 > â€¦, < ğ‘¥ğ‘› , ğ‘¦ğ‘› >),
Objective Function: ğ‘“, learning rate: ğ›‚, max_iterations: ğ’Šğ’ğ’‚ğ’™ , epsilon : ğœ€
Output : ğ‘¤, b.
{
Randomly initialize the elements of ğ‘¤ and b.
ğ‘™ğ‘œğ‘™ğ‘‘ = 0
for 1 to ğ’Šğ’ğ’‚ğ’™ :
for each < ğ‘¥ğ‘– , ğ‘¦ğ‘– > âˆˆ ğ·
ğœ•ğ‘™

ğ’˜ â† ğ’˜ âˆ’ ğ›‚ ğœ•ğ‘¤ğ‘–
ğœ•ğ‘™

ğ’ƒ â† ğ’ƒ âˆ’ ğ›‚ ğœ•ğ‘ğ‘–
endfor
calculate ğ‘™ = Ïƒğ‘›ğ‘–=0 ğ‘¦ğ‘– âˆ’ (ğ‘¤ âˆ™ ğ‘¥ğ‘– + ğ‘) 2
if |ğ‘™ - ğ‘™ğ‘œğ‘™ğ‘‘ | < ğœ€ :
break;
else
ğ‘™ğ‘œğ‘™ğ‘‘ = ğ‘™
endfor
Return ğ’˜, ğ’ƒ.
}

32

ê¸°íƒ€: ì ì ˆí•œ learning rateì˜ ì¤‘ìš”ì„±

ì´ˆê¸°ê°’

ì´ˆê¸°ê°’

Learning rate ğ›‚ê°€ ì‘ì€ ê²½ìš°:
Trainingì´ ë§¤ìš° ëŠë¦¬ê²Œ ì§„í–‰.

Learning rate ğ›‚ê°€ í° ê²½ìš°:
ë°œì‚° (Divergence)í•¨.

33

Local minimum (maximum), Global minimum (maximum)

Image source : https://deepai.org/machine-learning-glossary-and-terms/stochastic-gradient-descent

34

Gradient descent for multivariate
Linear Regression
35

Multivariate Linear Regression (ì¼ë°˜ Linear regression)
ï‚– Simple Linear Regressionì—ì„œ feature ğ’™ì˜ ê°’ì´ scalarê°€ ì•„ë‹Œ, ë²¡í„° (ê°’ì´ ì—¬ëŸ¬ ê°œ ì¡´ì¬
(Multivariate))ì¸ ì¼ë°˜í™”ëœ ë²„ì „ì„ ìƒê°í•´ë³´ì.
ï‚– Outcome variable ì„ í•˜ë‚˜ ì´ìƒì˜ feature variableë“¤ì˜ ì„ í˜• ê²°í•©ìœ¼ë¡œ ì˜ˆì¸¡í•˜ëŠ” Regression.

ğ‘¦ = Ïƒğ‘˜ğ‘–=1 ğ‘¤ğ‘– ğ‘¥ğ‘– + ğ‘
y : Outcome variable (ì˜ˆì¸¡í•˜ë ¤ëŠ” ë³€ìˆ˜).
ğ’™ğŸ , ğ’™ğŸ ,â€¦, ğ’™ğ’Œ : Feature variables. (ì´í•˜ ì¤„ì—¬ì„œ featureë¼ í•œë‹¤.)
ğ’˜ğŸ , ğ’˜ğŸ ,â€¦, ğ’˜ğ’Œ : Weights for each feature variable.
b : bias

Feature 3ê°œì¸ Linear Regression ì˜ˆ)

y = 1.5 * ğ’™ğŸ - 0.1* ğ’™ğŸ + 0.5 ğ’™ğŸ‘ + b
y : ë³µë¶€ ë‘˜ë ˆ
ğ’™ğŸ : ëª¸ë¬´ê²Œ
ğ’™ğŸ : ìš´ë™ëŸ‰
ğ’™ğŸ‘ : ì‹ì‚¬ëŸ‰

36

Multivariate Linear Regression : Objective Function
ï‚– ë‹¤ìŒê³¼ ê°™ì€ MSEë¥¼ Objective Functionìœ¼ë¡œ ì‚¬ìš©í•œë‹¤.
2
1 ğ‘›
ğ‘˜
MSE = Ïƒğ‘–=1 ğ‘¦ğ‘— âˆ’ (Ïƒğ‘–=1 ğ‘¤ âˆ™ ğ‘¥ğ‘– + ğ‘ )
ğ‘›
ğ‘¥ğ‘– = <ğ‘¥ğ‘–1 , ğ‘¥ğ‘–2 , â€¦ , ğ‘¥ğ‘–ğ‘˜ >,
ğ‘¤ = <ğ‘¤1 , ğ‘¤2 , â€¦ , ğ‘¤ğ‘˜ >,

Gradient Descent Update:
ğœ•MSE(ğ‘¤, ğ‘)
ğ‘¤ =ğ‘¤âˆ’ğ‘
ğœ•ğ‘¤
ğœ•MSE(ğ‘¤, ğ‘)
ğœ•MSE(ğ‘¤, ğ‘) ğœ•MSE(ğ‘¤, ğ‘)
ğœ•MSE(ğ‘¤, ğ‘)
=<
,
,â€¦,
>
ğœ•ğ‘¤
ğœ•ğ‘¤1
ğœ•ğ‘¤2
ğœ•ğ‘¤ğ‘˜

37

ì—°ìŠµë¬¸ì œ
ï‚– ğ‘¦ğ‘– = ğ‘¤ âˆ™ ğ‘¥ğ‘– + ğ‘
2
1 ğ‘›
ğ‘˜
ï‚– MSE(ğ‘¤) = Ïƒğ‘–=1 ğ‘¦ğ‘— âˆ’ (Ïƒğ‘–=1 ğ‘¤ âˆ™ ğ‘¥ğ‘– + ğ‘ )
ğ‘›

ï‚– ë°ì´í„°ê°€ ğ‘› = 1, ğ‘¥1 = < 1, 2 >, ğ‘¦1 = âˆ’1

ï‚– ğ‘¤ì˜ í˜„ì¬ê°’ ğ‘¤ (0) ì´ <0.5, 0.5>, ğ‘ = 1, learning rate ğ›¼ = 0.1ì˜ ê²½ìš°
ï‚– Stochastic gradient decentë¥¼ í™œìš©í•˜ì—¬ 1íšŒ updateí•œ ğ‘¤ (1) ê°’ì„ êµ¬í•˜ë¼.

38

Usage 1 : Prediction

39

Prediction by using linear regression.
ï‚– ì‚¬ìš© ì˜ˆ 1. Prediction:

ï‚– Linear Regressionì„ í•™ìŠµí•˜ì˜€ë‹¤ë©´, í•™ìŠµí•œ ëª¨ë¸ì„ í™œìš©í•  ìˆ˜ ìˆëŠ” ë°©ë²• ì¤‘ í•˜ë‚˜ë¡œ,
Training dataì…‹ì— ì—†ë˜ ìƒˆ ë°ì´í„°ì˜ feature xì— ëŒ€í•´, í•™ìŠµí•œ Linear Regression ëª¨ë¸ì„ ì‚¬
ìš©í•˜ì—¬ outcome variable yë¥¼ ì˜ˆì¸¡í•  ìˆ˜ ìˆë‹¤.
ï‚– ê°„ë‹¨íˆ ë§í•˜ë©´ y ê°’ì„ ì•Œ ìˆ˜ ì—†ëŠ” ìƒˆ ë°ì´í„°ì— ëŒ€í•´ yë¥¼ ì˜ˆì¸¡í•  ìˆ˜ ìˆë‹¤.

40

Apply simple linear regression on real data by using scikit-learn
ï‚– This code is based on the code presented in
ï‚– https://scikit-learn.org/stable/auto_examples/linear_model/plot_ols.html
import matplotlib.pyplot as plt
import numpy as np
from sklearn import linear_model
from sklearn.metrics import mean_squared_error, r2_score
from DataLoader import DataLoader
def simple_linear_regression_example():
# Load the height, weight dataset
height_x, weight_y = DataLoader.load_height_weight("../data/weight-height.csv")
num_data = len(height_x)
train_ratio = 0.8
num_train = int(num_data * train_ratio)
num_test = num_data - num_train

41

Apply simple linear regression on real data by using scikit-learn (Contâ€™d)
# Split the data into training/testing sets
height_x_train = height_x[:-num_test].reshape(-1, 1)
height_x_test = height_x[num_train:].reshape(-1, 1)
weight_y_train = weight_y[:-num_test].reshape(-1, 1)
weight_y_test = weight_y[num_train:].reshape(-1, 1)
# Create linear regression object
regr = linear_model.LinearRegression()
# Train the model using the training sets
regr.fit(height_x_train, weight_y_train)
# Make predictions using the testing set
weight_y_pred = regr.predict(height_x_test)

42

Apply simple linear regression on real data by using scikit-learn (Contâ€™d)
# The coefficients
print('Coefficients: \n', regr.coef_)
# The mean squared error
print('Mean squared error: %.2f'
% mean_squared_error(weight_y_test, weight_y_pred))
# The coefficient of determination: 1 is perfect prediction
print('Coefficient of determination: %.2f'
% r2_score(weight_y_test, weight_y_pred))
# Plot outputs
plt.scatter(height_x_test, weight_y_test, color='black')
plt.plot(height_x_test, weight_y_pred, color='blue', linewidth=3)
plt.xticks(np.arange(130, 205, step=5))
plt.yticks(np.arange(30, 105, step=5))

plt.show()

if __name__ == '__main__':
simple_linear_regression_example()

43

Apply simple linear regression on real data by using scikit-learn (Contâ€™d)

44

Prediction ê²°ê³¼ë¥¼ í•´ì„í•  ë•Œ ì£¼ì˜ì .

45

ì£¼ì˜í•´ì•¼í•  ì 
ï‚– ë°ì´í„°ê°€ ì–´ë–»ê²Œ ë¶„í¬í•˜ê³  ìˆê±´ Linear Regressionìœ¼ë¡œ model parameter(w, b)
ëŠ” ê³„ì‚°ëœë‹¤.
ï‚– ë‹¬ë¦¬ ë§í•˜ë©´ ì–´ë–¤ ë°ì´í„°ë„ Linear Regression Modelì„ í•™ìŠµí•  ìˆ˜ ìˆê³ , ì´ë¥¼
ì‚¬ìš©í•´ì„œ ì˜ˆì¸¡í•  ìˆ˜ ìˆë‹¤.
ï‚– ë‹¤ë§Œ, Modelì„ ì‚¬ìš©í•œ ì˜ˆì¸¡ì´ ìœ ìš©í•œì§€ ìœ ìš©í•˜ì§€ ì•Šì€ì§€ì˜ ì°¨ì´ê°€ ìˆì„ ë¿ì´
ë‹¤.

46

Linear Regressionìœ¼ë¡œ ì˜ˆì¸¡ì´ ê°€ëŠ¥í•œ ë°ì´í„° ë¶„í¬ vs ê°€ëŠ¥í•˜ì§€ ì•Šì€ ë¶„í¬
Linear Regressionìœ¼ë¡œ ì˜ˆì¸¡ ê°€ëŠ¥í• ì§€ ê·¸ë ‡ì§€ ì•Šì„ì§€ì˜ PointëŠ” ë¬´ì—‡ì¼ê¹Œ?
1.00
0.80

VS

VS 0.60
0.40

0.20
0.00
0.00

Randolph, Justus. (2007).
Multidisciplinary methods
in educational technology
research and
development.

0.20

0.40

0.60

0.80

https://www.statisticshowto.com/quadra
tic-regression/

47

1.00

Linear Regressionìœ¼ë¡œ ì˜ˆì¸¡ì´ ê°€ëŠ¥í•œ ë°ì´í„° ë¶„í¬ vs ê°€ëŠ¥í•˜ì§€ ì•Šì€ ë¶„í¬
(Contâ€™d)

Anscombe's quartet

Image source : https://www.mit.edu/~6.s085/notes/lecture3.pdf

Anscombe's quartetì€ í†µê³„ ì§€í‘œëŠ” ìœ ì‚¬í•˜ì§€ë§Œ ì‹¤ì œ ë°ì´í„° ë¶„í¬ëŠ” ë§¤ìš° ë‹¤ë¥¸ 4ê°œì˜ ë°ì´í„°ì…‹.
ê° ë°ì´í„°ì…‹ì€ 11ê°œì˜ (x, y) ì¢Œí‘œë¡œ ì´ë£¨ì–´ì§„ë‹¤.
1973ë…„, í†µê³„í•™ìì¸ í”„ë€ì‹œìŠ¤ ì•¤ìŠ¤ì»´(Francis Anscombe)ì´ ë°ì´í„° ë¶„ì„ ì „ 1) ì‹œê°í™”ì˜ ì¤‘ìš”ì„±ê³¼ 2) íŠ¹ì´ì¹˜ ë° ì£¼ ì˜í–¥
ê´€ì¸¡ê°’(influential observation)ì˜ ì˜í–¥ì„ ë³´ì—¬ì£¼ê¸° ìœ„í•´ ë§Œë“¤ì—ˆë‹¤. (Wikipedia ë°œì·Œ)
48
* ì„ ì˜ w = 0.5, b = 0.1

ì°¸ê³  : Anscombe's quartet data.
Anscombe's quartet
I

II

III

IV

x

y

x

y

x

y

x

y

10.0

8.04

10.0

9.14

10.0

7.46

8.0

6.58

8.0

6.95

8.0

8.14

8.0

6.77

8.0

5.76

13.0

7.58

13.0

8.74

13.0

12.74

8.0

7.71

9.0

8.81

9.0

8.77

9.0

7.11

8.0

8.84

11.0

8.33

11.0

9.26

11.0

7.81

8.0

8.47

14.0

9.96

14.0

8.10

14.0

8.84

8.0

7.04

6.0

7.24

6.0

6.13

6.0

6.08

8.0

5.25

4.0

4.26

4.0

3.10

4.0

5.39

19.0

12.50

12.0

10.84

12.0

9.13

12.0

8.15

8.0

5.56

7.0

4.82

7.0

7.26

7.0

6.42

8.0

7.91

5.0

5.68

5.0

4.74

5.0

5.73

8.0

6.89

49

Model Evaluation :
ï‚– í•™ìŠµí•œ Linear Regression ëª¨ë¸ì´ ë°ì´í„°ì˜ ê²½í–¥ì„ ì œëŒ€ë¡œ í•™ìŠµí•˜ì˜€ëŠ”ì§€ ì–´ë–»
ê²Œ íŒë‹¨í•  ìˆ˜ ìˆëŠ”ê°€?
ï‚– ì˜ˆì¸¡ ê²°ê³¼ë¥¼ ì‹ ë¢°í•  ìˆ˜ ìˆëŠ”ì§€ íŒë‹¨í•  ìˆ˜ ìˆì–´ì•¼ í•œë‹¤.

ï‚– ë°©ë²• 1 : Evaluation Dataì— ëŒ€í•´ MSEë¥¼ ê³„ì‚°í•˜ì—¬ ì˜ˆì¸¡ì„ ë¯¿ì„ ìˆ˜ ìˆëŠ”ì§€ íŒë‹¨
í•œë‹¤.
ï‚– MSEì˜ í¬ê¸°ê°€ (ì‚¬ìš©ìê°€ ìƒê°í•˜ëŠ”) ì˜¤ì°¨ ë²”ìœ„ë‚´ë©´ ë¯¿ëŠ”ë‹¤.
ï‚– ì•„ë‹ˆë©´? ë‹¤ë¥¸ ëª¨ë¸ì„ ì°¾ì•„ì•¼í•œë‹¤.

ï‚– ë°©ë²• 2 : Residualì„ ì‚¬ìš©.

50

Model Evaluation : ğ’“ğŸ (Contâ€™d)
Linear regression ì— ì˜í•œ ì˜ˆì¸¡ Line.

ë°ì´í„°ì˜ yê°’ í‰ê·  ğ‘¦.
à´¤

Image source : https://www.mit.edu/~6.s085/notes/lecture3.pdf

51

Model Evaluation : ğ’“ğŸ
ï‚– ìš°ë¦¬ê°€ feature xì™€ output yì˜ ê·¸ë˜í”„ë¥¼ ê·¸ë ¸ì„ ë•Œ xì— ê´€ê³„ ì—†ì´ ëª¨ë“  y ê°’ì´ yì˜ í‰ê· ê°’ì— ê·¼
ì ‘í•´ ìˆë‹¤ë©´ ìš°ë¦¬ëŠ” xì™€ yëŠ” ë³„ ê´€ë ¨ ì—†ë‹¤ í•  ìˆ˜ ìˆë‹¤.
Linear regression ì— ì˜í•œ ì˜ˆì¸¡ Line.

ë°ì´í„°ì˜ yê°’ í‰ê·  ğ‘¦.
à´¤
ë°ì´í„°ê°€ í‰ê·  ì£¼ìœ„ì— ëŠ˜ì–´ì„œ ìˆë‹¤ë©´?

Image source : https://www.mit.edu/~6.s085/notes/lecture3.pdf

52

Model Evaluation : ğ’“ğŸ (Contâ€™d)
Linear regression ì— ì˜í•œ ì˜ˆì¸¡ Line.

ë°ì´í„°ì˜ yê°’ í‰ê·  ğ‘¦.
à´¤
ë°ì´í„°ê°€ í‰ê·  ì£¼ìœ„ì— ëŠ˜ì–´ì„œ ìˆë‹¤ë©´?

ï‚– ë”°ë¼ì„œ ì–´ë–¤ ë°ì´í„° iì˜ yê°’ì„ ğ‘¦ğ‘– ë¼ í•  ë•Œ ğ‘¦ğ‘– ì™€ yê°’ í‰ê·  ğ‘¦à´¤ ì˜ ì°¨ì´ ğ‘¦ğ‘– âˆ’ ğ‘¦à´¤ ë¥¼ ê³„ì‚°í•œë‹¤ê³  ìƒê°í•´ ë³´ì.

ï‚– ìœ„ ë…¼ì˜ì— ê·¼ê±°í•˜ë©´, xì™€ yê°€ ê´€ë ¨ì´ í¬ë ¤ë©´ ìµœì†Œí•œ ğ‘¦ğ‘– âˆ’ ğ‘¦à´¤ ì°¨ì´ê°€ ì»¤ì•¼í•œë‹¤. (|ğ‘¦ğ‘– âˆ’ ğ‘¦|
à´¤ ì´ ì»¤ì•¼ í•œë‹¤.)

53

Model Evaluation : ğ’“ğŸ (Contâ€™d)
ï‚– ê·¸ëŸ°ë° ìš°ë¦¬ê°€ ê´€ì‹¬ìˆëŠ” ê²ƒì€ Linear Regression modelì„ ì‚¬ìš©í•œ ì˜ˆì¸¡ì´ ì–¼ë§ˆë‚˜ ì˜ë˜ëŠ”ê°€ ì´
ë¯€ë¡œ,
ï‚– ğ‘¦ğ‘– âˆ’ ğ‘¦à´¤ ê³„ì‚° ê³¼ì •ì— modelì„ ì‚¬ìš©í•˜ì—¬ ì˜ˆì¸¡í•œ ì˜ˆì¸¡ê°’ ğ‘¦à·ğ‘– ë„ ë¼ì›Œ ë„£ëŠ”ë‹¤.
à´¥ = ğ’šà·ğ’Š âˆ’ ğ’š
à´¥ + ğ’šğ’Š âˆ’ ğ’šà·ğ’Š
ï‚– ğ’šğ’Š âˆ’ ğ’š
à´¥ ì˜ ì ˆëŒ€ê°’ì´ í´ìˆ˜ë¡ Modelì˜ y ì˜ˆì¸¡ê°’ê³¼ y í‰ê· ê°’ì˜ ì°¨ì´ê°€ í¬ë¯€ë¡œ,
ï‚– ìœ„ ì‹ ì˜¤ë¥¸ìª½ì˜ ğ’šà·ğ’Š âˆ’ ğ’š
xì™€ yì˜ ê´€ê³„ê°€ ìˆë‹¤ë©´ ì´ë¥¼ ëª¨ë¸ì—ì„œ ì˜ ì˜ˆì¸¡í•œë‹¤ëŠ” ê²ƒì´ ëœë‹¤.

ï‚– ìœ„ ì‹ ì˜¤ë¥¸ìª½ì˜ ğ’šğ’Š âˆ’ ğ’šà·ğ’Š ì˜ ì ˆëŒ€ê°’ì´ í´ìˆ˜ë¡ Modelì˜ y ì˜ˆì¸¡ê°’ê³¼ ì‹¤ì œ yê°’ì˜ ì°¨ì´ê°€ í¬ë¯€ë¡œ
ëª¨ë¸ì´ ì˜ˆì¸¡ì„ ì˜ ëª»í•œë‹¤ëŠ” ê²ƒì´ ëœë‹¤.

54

Model Evaluation : ğ’“ğŸ (Contâ€™d)
ï‚– ğ‘¦ğ‘– âˆ’ ğ‘¦à´¤ = ğ‘¦à·ğ‘– âˆ’ ğ‘¦à´¤ + ğ‘¦ğ‘– âˆ’ ğ‘¦à·ğ‘– ì„ ì°¨ì´ì˜ ì ˆëŒ€ê°’ì„ ê³ ë ¤í•  ìˆ˜ ìˆë„ë¡ ë³€ê²½í•˜ê³  ëª¨ë“  ë°ì´í„°ì—
ëŒ€í•´ ì¼ë°˜í™”í•˜ë©´, ì•„ë˜ì™€ ê°™ì€ ì‹ì´ ëœë‹¤.
ï‚– Ïƒğ‘›ğ‘–=1 ğ‘¦ğ‘– âˆ’ ğ‘¦à´¤ 2 = Ïƒğ‘›ğ‘–=1 ğ‘¦à·ğ‘– âˆ’ ğ‘¦à´¤ 2 + Ïƒğ‘›ğ‘–=1 ğ‘¦ğ‘– âˆ’ ğ‘¦à·ğ‘– 2
ï‚– ìœ ë„ ê³¼ì •ì€ ìƒëµ. ê´€ì‹¬ ìˆìœ¼ë©´ https://rpubs.com/beane/n3_1b ë¥¼ ì°¸ì¡°í•˜ë¼.

ï‚– Ïƒğ‘›ğ‘–=1 ğ‘¦ğ‘– âˆ’ ğ‘¦à´¤ 2 = Ïƒğ‘›ğ‘–=1 ğ‘¦à·ğ‘– âˆ’ ğ‘¦à´¤ 2 + Ïƒğ‘›ğ‘–=1 ğ‘¦ğ‘– âˆ’ ğ‘¦à·ğ‘– 2
ï‚– ì—ì„œ Ïƒğ‘›ğ‘–=1 ğ‘¦à·ğ‘– âˆ’ ğ‘¦à´¤ 2 í´ìˆ˜ë¡ Modelì´ ì˜ ì˜ˆì¸¡í•˜ê³ , Ïƒğ‘›ğ‘–=1 ğ‘¦ğ‘– âˆ’ ğ‘¦à·ğ‘– 2 ì´ ì‘ì„ìˆ˜ë¡Modelì´ ì˜ëª»
ì˜ˆì¸¡í•œë‹¤.
ï‚– ê·¸ëŸ°ë° ì•„ë¬´ë¦¬ Modelì´ ì˜ ì˜ˆì¸¡í•´ë„ Ïƒğ‘›ğ‘–=1 ğ‘¦ğ‘– âˆ’ ğ‘¦à´¤ 2 ë¥¼ ë„˜ì„ ìˆ˜ëŠ” ì—†ìœ¼ë¯€ë¡œ, Ïƒğ‘›ğ‘–=1 ğ‘¦ğ‘– âˆ’ ğ‘¦à´¤ 2 ë¡œ
ìœ„ ì‹ì˜ ì¢Œ/ìš°ë³€ì„ ë‚˜ëˆ„ì–´ ëª¨ë¸ ì˜ˆì¸¡ ì„±ëŠ¥ì„ Normalizationí•  ìˆ˜ ìˆë‹¤.

55

Model Evaluation : ğ’“ğŸ (Contâ€™d)
Ïƒğ‘›
à·ğ‘– âˆ’ğ‘¦à´¤ 2
ğ‘–=1 ğ‘¦

ï‚– 1 = Ïƒğ‘›

à´¤ 2
ğ‘–=1 ğ‘¦ğ‘– âˆ’ğ‘¦

Ïƒğ‘›
ğ‘¦ğ‘– 2
ğ‘–=1 ğ‘¦ğ‘– âˆ’à·

+ Ïƒğ‘›

à´¤
ğ‘–=1 ğ‘¦ğ‘– âˆ’ğ‘¦

ëª¨ë¸ì˜ ì˜ˆì¸¡ë ¥

2

ì‹¤ì œ ë°ì´í„° ì— ëŒ€í•œ
ì˜ˆì¸¡ ì˜¤ì°¨

Ïƒğ‘›
Ïƒğ‘›
à·ğ‘– âˆ’ğ‘¦à´¤ 2
ğ‘¦ğ‘– 2
ğ‘–=1 ğ‘¦
ğ‘–=1 ğ‘¦ğ‘– âˆ’à·
ğŸ
ğ’“ = Ïƒğ‘› ğ‘¦ âˆ’ğ‘¦à´¤ 2 ì´ë¼ í•˜ë©´, Ïƒğ‘› ğ‘¦ âˆ’ğ‘¦à´¤ 2 ëŠ” ğŸ âˆ’ ğ’“ğŸ ë¡œ í‘œí˜„ë¨.
ğ‘–=1 ğ‘–
ğ‘–=1 ğ‘–

0 <= ğ‘Ÿ 2 <=1 , ğ‘Ÿ 2 ì´ 1ì— ê·¼ì ‘í•  ìˆ˜ë¡ ëª¨ë¸ í‘œí˜„ë ¥ì´ ì¢‹ë‹¤.
Training data ë¿ë§Œ ì•„ë‹ˆë¼ Evaluation dataì— ëŒ€í•´ì„œë„ ğ‘Ÿ 2 ë¥¼ ê³„ì‚°í•´ì„œ íŒë‹¨í•´ì•¼ í•œë‹¤.

56

Regressionì„ ì‚¬ìš©í•œ ì˜ˆì¸¡ì— ìˆì–´ì„œì˜ ì£¼ì˜ì :
Interpolation Vs. Extrapolation : Interpolation

Weight (kg)

Interpolation (ë³´ê°„ í˜¹ì€ ë‚´ì‚½) : ì•Œê³  ìˆëŠ” ë‘ ê°œ ì´ìƒì˜ ë°ì´í„° ì‚¬ì´ì˜ ë°ì´í„°ë¥¼ ì¶”ì¸¡í•˜ëŠ” ê²ƒ.
ì§ì„ ì—ì„œ ì¶”ì¸¡í•˜ë ¤ëŠ” ë°ì´í„°ì˜ ì¢Œì¸¡ê³¼ ìš°ì¸¡ì˜ ë°ì´í„°ë¥¼ ëª¨ë‘ ì•Œê³  ìˆëŠ” ê²½ìš°.
120
110
100
90
80
70
60
50
40
30
20
10
0

í‚¤ê°€ 187 cmì¸
ì‚¬ëŒì˜ ëª¸ë¬´ê²ŒëŠ”?

150

160

170

180

Height (cm)

190

200

57

Regressionì„ ì‚¬ìš©í•œ ì˜ˆì¸¡ì— ìˆì–´ì„œì˜ ì£¼ì˜ì :
Interpolation Vs. Extrapolation : Extrapolation

Weight (kg)

Extrapolation (ì™¸ì‚½) : ì•Œê³  ìˆëŠ” ë°ì´í„° ë²”ìœ„ ë°–ì˜ ë°ì´í„°ë¥¼ ì¶”ì¸¡í•˜ëŠ” ê²ƒ.
ì§ì„ ì—ì„œ ì¶”ì¸¡í•˜ë ¤ëŠ” ë°ì´í„°ì˜ ì¢Œì¸¡ í˜¹ì€ ìš°ì¸¡ì˜ ë°ì´í„°ë¥¼ í•œìª½ë§Œ ì•Œê³  ìˆëŠ” ê²½ìš°.
120
110
100
90
80
70
60
50
40
30
20
10
0

150
í‚¤ê°€ 155 cmì¸
ì‚¬ëŒì˜ ëª¸ë¬´ê²ŒëŠ”?

í‚¤ê°€ 200 cmì¸
ì‚¬ëŒì˜ ëª¸ë¬´ê²ŒëŠ”?

160

170

180

Height (cm)

190

200

58

Weight (kg)

Regressionì„ ì‚¬ìš©í•œ ì˜ˆì¸¡ì— ìˆì–´ì„œì˜ ì£¼ì˜ì :
Exploration í•  ê²½ìš°ëŠ” ì˜ˆì¸¡ ê²°ê³¼ê°€ ì‹¤ì œì™€ ë‹¤ë¥¼ í™•ë¥ ì´ ë” ë†’ë‹¤.
120
110
100
90
80
70
60
50
40
30
20
10
0

150
í‚¤ê°€ 155 cmì¸
ì‚¬ëŒì˜ ëª¸ë¬´ê²ŒëŠ”?
(60 kg ì „í›„?)

í‚¤ê°€ 200 cmì¸
ì‚¬ëŒì˜ ëª¸ë¬´ê²ŒëŠ”?
(105 kg ì „í›„?)
160

170

180
190
200
Height (cm) í‚¤ê°€ 187 cmì¸
ì‚¬ëŒì˜ ëª¸ë¬´ê²ŒëŠ”?
ì£¼ìœ„ ë°ì´í„°ë¥¼ ë³´ë©´
ì–´ëŠì •ë„ ì •ë‹µì´ ì˜ˆì¸¡ì´
ëœë‹¤.

59

ê·€ë‚©ì¶”ë¡ ì˜ ë¬¸ì œì 

ëŸ¬ì…€ì˜ ë‹­
The man who has fed the chicken every day throughout
its life at last wrings its neck instead.

Image source : https://www.mit.edu/~6.s085/notes/lecture3.pdf

60

Linear Regression
Training : Regularization

61

Overfitting
ï‚– Overfitting
ï‚– ëª¨ë¸ì´ Training dataë¥¼ í•„ìš”ì´ìƒìœ¼ë¡œ ì •í™•í•˜ê²Œ ì˜ˆì¸¡í•˜ë„ë¡ í•™ìŠµë˜ì–´ ëª¨ë¸ì˜
Generalization ëŠ¥ë ¥ì´ ì €í•˜ë˜ëŠ” ê²ƒ.
ï‚– Training dataì— ëŒ€í•´ì„œëŠ” ì™„ë²½í•œ ì˜ˆì¸¡ì„±ëŠ¥ì„ ë³´ì´ë‚˜ Test dataì— ëŒ€í•´ì„œëŠ” ì˜ˆì¸¡ ì„±ëŠ¥
ì´ ë‚®ë‹¤.
ï‚– ì¼ë°˜ì ìœ¼ë¡œ
ï‚– (1) Training dataì˜ ê·œëª¨ê°€ ì‘ê±°ë‚˜,
ï‚– (2) ëª¨ë¸ì˜ í‘œí˜„ë ¥ì´ dataì˜ ë¶„í¬ ê²½í–¥ì— ë¹„í•´ ë›°ì–´ë‚œ ê²½ìš°ì— ì¼ì–´ë‚œë‹¤.

62

Overfitting : Modelì˜ í‘œí˜„ë ¥ (Representative Power)
ï‚– Modelâ€™s Representative Power:
ï‚– Modelì´ í‘œí˜„í•  ìˆ˜ ìˆëŠ” ë³µì¡ë„.
ï‚– Modelì´ ê¸°ë°˜ìœ¼ë¡œ í•˜ëŠ” ì´ë¡ ì— ë”°ë¼ í‘œí˜„í•  ìˆ˜ ìˆëŠ” ìµœëŒ€ ë³µì¡ë„ê°€ ì •í•´ì§„ë‹¤.
ï‚– ì˜ˆ 1) Linear Regression : ì„ í˜• ëª¨ë¸, (ì œí•œëœ) ë¹„ì„ í˜• ëª¨ë¸ í‘œí˜„
ï‚– ì˜ˆ 2) Multi-Layer Perceptron : ê±°ì˜ ëŒ€ë¶€ë¶„ì˜ ë¹„ì„ í˜• ëª¨ë¸ í‘œí˜„
ï‚– ê°™ì€ Modelì—ì„œëŠ” ì¼ë°˜ì ìœ¼ë¡œ modelì˜ parameter ìˆ˜ê°€ ëŠ˜ì–´ë‚  ìˆ˜ë¡ í‘œí˜„ë ¥ì´ ì¦ê°€
í•œë‹¤.
ï‚– ì˜ˆ) Simple Linear Regression VS. Multi-variate Linear Regression
ï‚– ë¬¸ì œ) Linear Regressionì˜ model parameterëŠ”?

63

Overfitting ì˜ˆ
Training dataë¡œ ë¶€í„° ì˜ˆì¸¡ë˜ëŠ” ë°ì´í„°ì˜ ë¶„í¬ëŠ”?

y

x

64

Overfitting ì˜ˆ (Contâ€™d)
Simple Linear Regression : y = wx + b

y

x

65

Overfitting â€“ Example
"""
=========================================================
Linear Regression Overfit Example
=========================================================
"""
import matplotlib.pyplot as plt
from sklearn import linear_model
from sklearn.metrics import mean_squared_error, r2_score
from DataLoader import DataLoader
import numpy as np

def print_coef(coef):
coef_array = coef.flatten()
for val in coef_array:
print("%.4f" % val, end="\t")
print("")

def make_6_order_feature_vecs(x):
x1 = x
x2 = x * x
x3 = x2 * x
x4 = x3 * x
x5 = x4 * x
x6 = x5 * x

x1 = x1.reshape(-1, 1)
x2 = x2.reshape(-1, 1)
x3 = x3.reshape(-1, 1)
x4 = x4.reshape(-1, 1)
x5 = x5.reshape(-1, 1)
x6 = x6.reshape(-1, 1)
features = np.concatenate((x6, x5, x4, x3, x2, x1), axis=1)
return features

66

def linear_regression_overfit_example():
# Load the wine features, wine quality dataset
x, y = DataLoader.load_overfit_example()
x_train = x.reshape(-1, 1)
y_train = y.reshape(-1, 1)
regr = linear_model.LinearRegression()
# Train the model using the training sets
regr.fit(x_train, y_train)
# Make predictions using the testing set
x_min = 0
x_max = 10
step = 0.2
test_x = np.arange(x_min, x_max, step).reshape(-1, 1)
y_pred = regr.predict(test_x)

# Plot outputs
plt.scatter(x_train, y_train, color='black')
plt.xticks(np.arange(0, 10, step=1))
plt.yticks(np.arange(6, 12, step=1))
plt.ylim([5, 13])
plt.title('Training data')
plt.show()
# Plot outputs
plt.scatter(x_train, y_train, color='black')
plt.plot(test_x, y_pred, color='blue', linewidth=3)
plt.xticks(np.arange(0, 10, step=1))
plt.yticks(np.arange(6, 12, step=1))
plt.ylim([5, 13])
plt.title('One Feature Linear Regression')
plt.show()

67

features = make_6_order_feature_vecs(x)
features = features.reshape(-1, 6)
regr_multi = linear_model.LinearRegression()
regr_multi.fit(features, y_train)
# Make predictions using the testing set
x_min = 0
x_max = 10
step = 0.2
test_x = np.arange(x_min, x_max, step).reshape(-1, 1)
test_features = make_6_order_feature_vecs(test_x)
y_pred = regr_multi.predict(test_features)
print_coef(regr_multi.coef_)
# Plot outputs
plt.scatter(x_train, y_train, color='black')
plt.plot(test_x, y_pred, color='blue', linewidth=3)
plt.xticks(np.arange(0, 10, step=1))
plt.yticks(np.arange(6, 12, step=1))
plt.ylim([5, 13])
plt.title('Multi-Feature (6 order x ) with Regularization W = 0')
plt.show()

# L2 Regularization W = 0.1
ridge = linear_model.Ridge(alpha=0.1)
ridge.fit(features, y_train)
y_pred = ridge.predict(test_features)
print_coef(ridge.coef_)
# Plot outputs
plt.scatter(x_train, y_train, color='black')
plt.plot(test_x, y_pred, color='blue', linewidth=3)
plt.xticks(np.arange(0, 10, step=1))
plt.yticks(np.arange(6, 12, step=1))
plt.ylim([5, 13])
plt.title('Multi-Feature (6 order x ) with L2 Reg. Lambda = 0.1')
plt.show()
# L2 Regularization W = 1
ridge = linear_model.Ridge(alpha=1.0)
ridge.fit(features, y_train)
y_pred = ridge.predict(test_features)
print_coef(ridge.coef_)
# Plot outputs
plt.scatter(x_train, y_train, color='black')
plt.plot(test_x, y_pred, color='blue', linewidth=3)
plt.xticks(np.arange(0, 10, step=1))
plt.yticks(np.arange(6, 12, step=1))
plt.ylim([5, 13])
plt.title('Multi-Feature (6 order x ) with L2 Reg. Lambda = 1')
68
plt.show()

# L1 Regularization W = 0.1
lasso = linear_model.Lasso(alpha=0.1)
lasso.fit(features, y_train)
y_pred = lasso.predict(test_features)
print_coef(lasso.coef_)
# Plot outputs
plt.scatter(x_train, y_train, color='black')
plt.plot(test_x, y_pred, color='blue', linewidth=3)
plt.xticks(np.arange(0, 10, step=1))
plt.yticks(np.arange(6, 12, step=1))
plt.ylim([5, 13])
plt.title('Multi-Feature (6 order x ) with L1 Reg. Lambda = 0.1')
plt.show()

# L1 Regularization W = 1
lasso = linear_model.Lasso(alpha=1.0)
lasso.fit(features, y_train)
y_pred = lasso.predict(test_features)
print_coef(lasso.coef_)
# Plot outputs
plt.scatter(x_train, y_train, color='black')
plt.plot(test_x, y_pred, color='blue', linewidth=3)
plt.xticks(np.arange(0, 10, step=1))
plt.yticks(np.arange(6, 12, step=1))
plt.ylim([5, 13])
plt.title('Multi-Feature (6 order x ) with L1 Reg. Lambda = 1')
plt.show()

if __name__ == '__main__':
linear_regression_overfit_example()

69

Overfitting ì˜ˆ (Contâ€™d) â€“ Representative Powerê°€ ì¦ê°€ëœ Linear Regression
ï‚– 6 feature Multi-variable linear regression :
ï‚– ë‹¤ìŒ Linear Regressionì—ì„œ
ï‚– ğ’š = ğ’˜ğŸ” ğ’™ğŸ” + ğ’˜ğŸ“ ğ’™ğŸ“ + ğ’˜ğŸ’ ğ’™ğŸ’ + ğ’˜ğŸ‘ ğ’™ğŸ‘ + ğ’˜ğŸ ğ’™ğŸ + ğ’˜ğŸ ğ’™ğŸ + ğ’ƒ

ï‚– ì…ë ¥ vector ğ‘¥Ô¦ =< ğ‘¥6 , ğ‘¥5 , ğ‘¥4 , ğ‘¥3 , ğ‘¥2 , ğ‘¥1 > ì´ ë‹¤ìŒê³¼ ê°™ë‹¤ê³  í•˜ì.
ï‚– ğ‘¥6 = ğ‘¥ 6
ï‚– ğ‘¥5 = ğ‘¥ 5
ï‚– ğ‘¥4 = ğ‘¥ 4
ï‚– ğ‘¥3 = ğ‘¥ 3
ï‚– ğ‘¥2 = ğ‘¥ 2
ï‚– ğ‘¥1 = ğ‘¥

70

Overfitting ì˜ˆ (Contâ€™d) â€“ Representative Powerê°€ ì¦ê°€ëœ Linear Regression
6-order Linear Regression (Non-Linear Function):
ğ’š = ğ’˜ğŸ” ğ‘¥ 6 + ğ’˜ğŸ“ ğ‘¥ 5 + ğ’˜ğŸ’ ğ‘¥ 4 + ğ’˜ğŸ‘ ğ‘¥ 3 + ğ’˜ğŸ ğ‘¥ 2 + ğ’˜ğŸ ğ‘¥ + ğ’ƒ

Training Dataì— ëŒ€í•´ì„œëŠ” ê±°ì˜
ì™„ë²½íˆ yë¥¼ ì˜ˆì¸¡í•  ìˆ˜ ìˆë‹¤.

y

ê·¸ëŸ¬ë‚˜ ì´ Modelì„ ì‚¬ìš©í•˜ì—¬
ì˜ˆì¸¡í•˜ë©´?

x

71

ì™œ ì´ëŸ° ì¼ì´? Modelì´ í•„ìš” ì´ìƒìœ¼ë¡œ ë¨¸ë¦¬ê°€ ì¢‹ê¸° ë•Œë¬¸
ï‚– ë¨¸ë¦¬ê°€ ì¢‹ë‹¤

ï‚– = Model Representative Powerê°€ ë›°ì–´ë‚˜ë‹¤.
ï‚– = Model Parameterê°€ í•„ìš” ì´ìƒìœ¼ë¡œ ë§ë‹¤.

ï‚– Model Parameterì— í•˜ë‚˜í•˜ë‚˜ì˜ Training dataì˜ íŠ¹ì§•ì„ ì™¸ìš¸ ìˆ˜ ìˆë‹¤.

ï‚– ì£¼ì–´ì§„ ë§(training data)ì„ ë‹¨ìˆœí•˜ê²Œ ì„¤ëª…í•˜ë ¤ë©´, ë§ì˜ ì£¼ìš” íŠ¹ì§•ë§Œì„ ì¶”ì¶œí•˜ì—¬ ì„¤ëª…í•œë‹¤.
ï‚– => Generalization
ï‚– ëª©ì´ ê¸¸ê³ , êµ½ ìˆëŠ” ë‹¤ë¦¬ê°€ ë„¤ ê°œ ì´ê³ , ëª© ìœ„ì— ê°ˆê¸°ê°€ ìˆê³ , ë’¤ì— ê¼¬ë¦¬ê°€ ìˆë‹¤. ë°°ëŠ” ë³¼ë¡
í•˜ë‹¤.
ï‚– ëŒ€ë¶€ë¶„ì˜ ë§ (training dataì— ë‚˜íƒ€ë‚˜ì§€ ì•Šì€ data)ì€ ìœ„ì˜ íŠ¹ì§•ì„ ê°–ê³  ìˆë‹¤.
ï‚– ì£¼ì–´ì§„ ë§(training data) í•˜ë‚˜í•˜ë‚˜ë¥¼ ì™¸ìš¸ ìˆ˜ ìˆë‹¤ë©´, í•´ë‹¹ ë§ì˜ ëª¨ë“  íŠ¹ì§•ì„ ì„¸ì„¸í•˜ê²Œ ì„¤ëª…í• 
ìˆ˜ ìˆë‹¤.
ï‚– => Memorization (Overfitting)
ï‚– ëª©ì´ ê¸¸ê³ , êµ½ ìˆëŠ” ë‹¤ë¦¬ê°€ ë„¤ ê°œ ì´ê³ , ëª© ìœ„ì— ê°ˆê¸°ê°€ ìˆê³ , ë’¤ì— ê¼¬ë¦¬ê°€ ìˆë‹¤. ë°°ëŠ” ë³¼ë¡
í•œë°ë‹¤ê°€ ì™¼ìª½ ê·€ì— ì ì´ ë‘ ê°œ ìˆê³ , ë°°ì— ì¤„ë¬´ëŠ¬ê°€ ìˆìœ¼ë©°, ì´ê°€ í•˜ë‚˜ ì—†ê³ ,...
ï‚– ë‹¤ë¥¸ ë§ (training dataì— ë‚˜íƒ€ë‚˜ì§€ ì•Šì€ data) ì€ ìœ„ì˜ íŠ¹ì§•ì„ ê°€ì§€ì§€ ì•ŠëŠ” ê²½ìš°ê°€ ë§ë‹¤.

72

Occamâ€™s Razor : Simpler one is better
Two scientific theories which explain the same
situation equally well, the simpler one is
preferred.

Image source: https://examples.yourdictionary.com/examples-ofoccam-s-razor.html

Overfittingì„ í”¼í•˜ê¸° ìœ„í•´ì„œëŠ” Training setì— ëŒ€í•œ Errorê°€ ë¹„ìŠ·í•œ ê²½ìš°(explain the same situation
equally well), parameter ìˆ˜ê°€ ë” ì ì€ Modelì´ ë” ì¢‹ë‹¤. (the simpler one is preferred)

73

ì–´ë ¤ìš´ ì : Model Parameter ìˆ˜ë¥¼ ì¤„ì´ëŠ” ê²ƒì€ ë•Œë•Œë¡œ ì‰½ì§€ ì•Šë‹¤.
ï‚– ì˜ˆ) ë³µë¶€ ë‘˜ë ˆë¥¼ ì˜ˆì¸¡í•˜ëŠ”ë° ìˆì–´ì„œ <ëª¸ë¬´ê²Œ, ìš´ë™ëŸ‰, ì‹ì‚¬ëŸ‰, í‚¤> ë¥¼ Feature
ë¡œ ì‚¬ìš©í•˜ëŠ”ë°, ì´ ì¤‘ ë‘ ê°œë¥¼ ì¤„ì—¬ì•¼ í•œë‹¤ê³  í•˜ì. ë¬´ì—‡ì„ ì¤„ì¼ê¹Œ?
ï‚– ë‚¨ê¸¸ ì¤‘ìš”í•œ Feature 2ê°œë¥¼ ì•Œê¸° ì–´ë µë‹¤.
ï‚– ì˜ ëª¨ë¥´ë‹ˆê¹Œ Machine Learningìœ¼ë¡œ í•™ìŠµí•œë‹¤.
ï‚– ë‹¤ ë³µë¶€ ë‘˜ë ˆì™€ ì—°ê´€ì´ ìˆì–´ ë³´ì¸ë‹¤.
ï‚– ëª¨ë“  featureë¥¼ ì‚¬ìš©í•˜ê³  ì‹¶ë‹¤.

ï‚– Feature ìˆ˜ë¥¼ ì¤„ì´ì§€ ì•Šê³  Overfittingì„ ë°©ì§€í•  ìˆ˜ ìˆëŠ” ë°©ë²•ì´ ì—†ì„ê¹Œ?

74

Regularization
ï‚– Model Parameterì˜ ê°’ì´ ê·¹ë‹¨ì ì¸ ë¶„í¬ë¥¼ ê°™ì§€ ì•Šë„ë¡ í•˜ì—¬ Overfittingì„ ë°©
ì§€í•˜ëŠ” ë°©ë²•.
ï‚– ì¼ë°˜ì ìœ¼ë¡œ Overfittingì´ ì¼ì–´ë‚  ê²½ìš° model parameterê°€ ê·¹ë‹¨ì ì¸ ê°’ì„ ê°€ì§€ëŠ” ê²½í–¥ì´
ìˆë‹¤.

6-order Linear Regression (Non-Linear Function):
ğ’š = ğ’˜ğŸ” ğ‘¥ 6 + ğ’˜ğŸ“ ğ‘¥ 5 + ğ’˜ğŸ’ ğ‘¥ 4 + ğ’˜ğŸ‘ ğ‘¥ 3 + ğ’˜ğŸ ğ‘¥ 2 + ğ’˜ğŸ ğ‘¥ + ğ’ƒ
w1
-0.0102

y

w2
0.326

w3
-4.1293

w4
w5
w6
26.3653 -88.8855 149.7337

x

ï‚– ì£¼ë¡œ ì•„ë˜ ë‘ Regularization ë°©ë²•ì´ ì£¼ë¡œ ì‚¬ìš©ëœë‹¤.
ï‚– L2 Regularization (Ridge Regularization),
ï‚– L1 Regularization (Lasso Regularization)

75

Regularization
ï‚– Objective Function ì— Regularization ì‹ì„ ì¶”ê°€í•œë‹¤.
2
ğ‘›
Ïƒ
ï‚– Objective Function : ğ‘™ğ‘œğ‘™ğ‘‘ = ğ‘–=1 ğ‘¦ğ‘— âˆ’ (ğ‘¤ âˆ™ ğ‘¥ğ‘– + ğ‘)
2
ï‚– New Objective Function: ğ‘™ğ‘›ew = Ïƒğ‘›ğ‘–=1 ğ‘¦ğ‘— âˆ’ (ğ‘¤ âˆ™ ğ‘¥ğ‘– + ğ‘) + Î» âˆ™ ğ‘¹
ï‚– Î» : Regularization Weight
ï‚– R : Regularization Equation (Sigma ë°–ì— ìˆìŒì— ìœ ì˜)

76

L2 Regularization
ï‚– L2 Regularization (Ridge Regularization)
ï‚– Model Parameter ë¥¼ Ñ² ë¼ í‘œê¸°í•˜ë©´, L2 Regularizationì€ Modelì´ Ñ² 22 ë¥¼ ìµœì†Œí™” í•˜ëŠ” Ñ²ë¥¼
í•™ìŠµí•˜ë„ë¡ í•œë‹¤.
ï‚– Ñ² 22 : Ñ² ì˜ Frobenius norm ì˜ ì œê³± : Ñ²ê°€ vectorë¼ë©´ vectorì˜ í¬ê¸°ì˜ ì œê³±
ï‚– Linear regressionì˜ ì‹ì´ ğ‘¦à·œğ‘– = ğ‘¤ âˆ™ ğ‘¥ğ‘– + ğ‘ ì´ê³ , ìš°ë¦¬ê°€ í•™ìŠµí•´ì•¼ í•  variableì´ ğ‘¤, ğ‘ ì´ë¯€ë¡œ,
ï‚– Linear regressionì˜ L2 Normì€ ğ‘¤ 2 + ğ‘ 2 = ğ‘¤ âˆ™ ğ‘¤ + ğ‘ 2 = ğ‘¤ 2 + ğ‘2 ì´ ëœë‹¤.
ï‚– ë”°ë¼ì„œ,
Î»
ï‚– New Objective Function: ğ‘™ğ‘›ew = Ïƒğ‘›ğ‘–=1 ğ‘¦ğ‘– âˆ’ (ğ‘¤ âˆ™ ğ‘¥ğ‘– + ğ‘) 2 + âˆ™ (ğ‘¤ 2 + ğ‘ 2 )
ğŸ
Î»
ï‚– ì˜ 2ëŠ” SGDë¥¼ ì‰½ê²Œ í•˜ê¸° ìœ„í•œ Trick. ì´ë¡ ìƒìœ¼ë¡œëŠ” Î»ë§Œ í‘œê¸°í•´ë„ ë¨.
2

77

Stochastic Gradient Descent (SGD) with L2 Regularization
ì£¼ì–´ì§„ < ğ‘¥ğ‘– , ğ‘¦ğ‘– > ì— ëŒ€í•´ SE (ğ‘™ğ‘– )ë¥¼ ğ‘¤ ë¡œ ë¯¸ë¶„í•  ê²½ìš°.
ì•„ë˜ ì‹ì—ì„œ vector ì‚¬ì´ì˜ ê³±ì€ dot productë¥¼ ëœ»í•œë‹¤.)
ğ‘™ğ‘– = ğ‘¦ğ‘– âˆ’ (ğ‘¤ âˆ™ ğ‘¥ğ‘– + ğ‘) 2 +

Î»
ğŸ

âˆ™ (ğ‘¤ 2 + ğ‘2 )

Î»
= ğ‘¦ğ‘–2 âˆ’ 2ğ‘¦ğ‘– âˆ™ ğ‘¤ âˆ™ ğ‘¥ğ‘– + ğ‘ + ğ‘¤ âˆ™ ğ‘¥ğ‘– + ğ‘ 2 + âˆ™ (ğ‘¤ 2 + ğ‘2 )
ğŸ

Î»
Î»
= ğ‘¦ğ‘–2 âˆ’ 2ğ‘¦ğ‘– âˆ™ ğ‘¤ âˆ™ ğ‘¥ğ‘– âˆ’ 2ğ‘¦ğ‘– ğ‘ + ğ‘¤ 2 âˆ™ ğ‘¥ğ‘–2 + 2ğ‘ âˆ™ ğ‘¤ âˆ™ ğ‘¥ğ‘– + ğ‘ 2 + âˆ™ ğ‘¤ 2 + âˆ™ ğ’ƒğŸ
ğŸ
ğŸ
Î»
Î»
= (ğ‘¥ğ‘–2 + ) âˆ™ ğ‘¤ 2 + 2ğ‘ âˆ™ ğ‘¥ğ‘– âˆ’ 2ğ‘¦ğ‘– âˆ™ ğ‘¥ğ‘– âˆ™ ğ‘¤ âˆ™ +(1 + )ğ‘ 2 âˆ’ 2ğ‘¦ğ‘– ğ‘ + ğ‘¦ğ‘–2 (ğ‘¤ ì— ëŒ€í•´ ì •ë¦¬)
ğŸ

ğŸ

ğœ•ğ‘™ğ‘–
Î»
= 2 âˆ™ (ğ‘¥ğ‘–2 + ) âˆ™ ğ‘¤ âˆ’2ğ‘¦ğ‘– âˆ™ ğ‘¥ğ‘– + 2ğ‘ âˆ™ ğ‘¥ğ‘–
ğœ•ğ‘¤
ğŸ

= âˆ’2 âˆ™ ğ‘¥ğ‘– âˆ™ ğ‘¦ğ‘– âˆ’ ğ‘¤ âˆ™ ğ‘¥ğ‘– âˆ’ ğ‘ + Î» âˆ™ ğ‘¤
= âˆ’2 âˆ™ ğ‘¥ğ‘– âˆ™ (ğ‘¦ğ‘– âˆ’ (ğ‘¤ âˆ™ ğ‘¥ğ‘– + ğ‘)) + Î» âˆ™ ğ‘¤
= âˆ’2 âˆ™ ğ‘¥ğ‘– âˆ™ (ğ‘¦ğ‘– âˆ’ ğ‘¦à·œğ‘– ) + Î» âˆ™ ğ‘¤

78

Stochastic Gradient Descent (SGD) with L2 Regularization (Contâ€™d)
ì£¼ì–´ì§„ < ğ‘¥ğ‘– , ğ‘¦ğ‘– > ì— ëŒ€í•´ SE (ğ‘™ğ‘– )ë¥¼ ğ‘¤ ìœ¼ë¡œ ë¯¸ë¶„í•  ê²½ìš°.
ì•„ë˜ ì‹ì—ì„œ vector ì‚¬ì´ì˜ ê³±ì€ dot productë¥¼ ëœ»í•œë‹¤.)
ğ‘™ğ‘– = ğ‘¦ğ‘– âˆ’ (ğ‘¤ âˆ™ ğ‘¥ğ‘– + ğ‘) 2 +

Î»
ğŸ

âˆ™ (ğ‘¤ 2 + ğ‘2 )

Î»
= ğ‘¦ğ‘–2 âˆ’ 2ğ‘¦ğ‘– âˆ™ ğ‘¤ âˆ™ ğ‘¥ğ‘– + ğ‘ + ğ‘¤ âˆ™ ğ‘¥ğ‘– + ğ‘ 2 + âˆ™ (ğ‘¤ 2 + ğ‘2 )
ğŸ

Î»
Î»
= ğ‘¦ğ‘–2 âˆ’ 2ğ‘¦ğ‘– âˆ™ ğ‘¤ âˆ™ ğ‘¥ğ‘– âˆ’ 2ğ‘¦ğ‘– ğ‘ + ğ‘¤ 2 âˆ™ ğ‘¥ğ‘–2 + 2ğ‘ âˆ™ ğ‘¤ âˆ™ ğ‘¥ğ‘– + ğ‘ 2 + âˆ™ ğ‘¤ 2 + âˆ™ ğ’ƒğŸ
ğŸ
ğŸ
= ? (ğ‘ ì— ëŒ€í•´ ì •ë¦¬)
ğœ•ğ‘™ğ‘–
=?
ğœ•ğ‘

79

L2 Regularizationì˜ íš¨ê³¼
ï‚– Overfitting ë°©ì§€
ï‚– Model parameterì´ Mean ğœ‡ = 0, Standard Deviation ğœ ì˜ ë¶„í¬ë¥¼ ê°€ì§„ë‹¤.
ï‚– = Model parameter ê°’ì´ ê·¹ë‹¨ì ìœ¼ë¡œ ì°¨ì´ë‚˜ì§€ ì•Šê³  ë¹„ìŠ·í•˜ì§€ë§Œ ì¡°ê¸ˆì”© ë‹¤ë¥¸ ê°’ì„ ê°€ì§„
ë‹¤.

80

L2 Regularizationì˜ íš¨ê³¼ (Contâ€™d)
ï‚– Overfitting ë°©ì§€
ï‚– Model parameterì´ Mean ğœ‡ = 0, Standard Deviation ğœ ì˜ ë¶„í¬ë¥¼ ê°€ì§„ë‹¤.
ï‚– = Model parameter ê°’ì´ ê·¹ë‹¨ì ìœ¼ë¡œ ì°¨ì´ë‚˜ì§€ ì•Šê³  ë¹„ìŠ·í•˜ì§€ë§Œ ì¡°ê¸ˆì”© ë‹¤ë¥¸ ê°’ì„ ê°€
ì§„ë‹¤.

L2 Lambda
No Reg.
0.1
1

w1
-0.0102
-0.0005
0

w2
0.326
0.0147
0.002

w3
-4.1293
-0.1559
-0.0263

w4
26.3653
0.7017
0.1258

w5
w6
-88.8855 149.7337
-1.0072
-0.6312
-0.1013
-0.0721

Mean
13.900
-0.180
-0.012

SD
43.968
0.611
0.082

81

L1 Regularization
ï‚– L1 Regularization (Lasso Regularization)
ï‚– Model Parameter ë¥¼ Ñ² ë¼ í‘œê¸°í•˜ë©´, L1 Regularizationì€ Modelì´ Ñ² 11 ë¥¼ ìµœì†Œí™” í•˜ëŠ” Ñ²ë¥¼
í•™ìŠµí•˜ë„ë¡ í•œë‹¤.
ï‚– Ñ² 11 : Ñ² Matrix elementì˜ ì ˆëŒ€ê°’ì˜ í•©
ï‚– Linear regressionì˜ ì‹ì´ ğ‘¦à·œğ‘– = ğ‘¤ âˆ™ ğ‘¥ğ‘– + ğ‘ ì´ê³ , ìš°ë¦¬ê°€ í•™ìŠµí•´ì•¼ í•  variableì´ ğ‘¤, ğ‘ ì´ë¯€ë¡œ,
ï‚– Linear regressionì˜ L1 Normì€ Ïƒğ‘˜ğ‘–=1 |ğ‘¤ğ‘– | + |ğ‘| ì´ ëœë‹¤. (ğ‘¤ =< ğ‘¤1 , ğ‘¤2 , â€¦ , ğ‘¤ğ‘˜ >)
ï‚– ë”°ë¼ì„œ,
ï‚– New Objective Function: ğ‘™ğ‘›ew = Ïƒğ‘›ğ‘–=1 ğ‘¦ğ‘– âˆ’ (ğ‘¤ âˆ™ ğ‘¥ğ‘– + ğ‘) 2 +Î»( Ïƒğ‘˜ğ‘–=1 |ğ‘¤ğ‘– | + |ğ‘|)

82

L1 Regularizationì˜ íš¨ê³¼
ï‚– Overfitting ë°©ì§€
ï‚– Sparseí•œ model parameterê°€ í•™ìŠµë¨.

ï‚– ëª‡ëª‡ ì¤‘ìš”í•œ weightì—ë§Œ ê°’ì´ ìˆê³  ì¤‘ìš”í•˜ì§€ ì•Šì€ parameterëŠ” ê°’ì´ 0ì— ê°€ê¹Œì›Œì§„ë‹¤.

83

L1 Regularizationì˜ íš¨ê³¼ (Contâ€™d)
ï‚– Overfitting ë°©ì§€
ï‚– Sparseí•œ model parameterê°€ í•™ìŠµë¨.

ï‚– ëª‡ëª‡ ì¤‘ìš”í•œ weightì—ë§Œ ê°’ì´ ìˆê³  ì¤‘ìš”í•˜ì§€ ì•Šì€ parameterëŠ” ê°’ì´ 0ì— ê°€ê¹Œì›Œì§„ë‹¤.

L1 Lambda
No Reg.
0.1
1

w1
-0.0102
0
0

w2
0.326
-0.0002
-0.0002

w3
-4.1293
-0.0009
0

w4
26.3653
0.0033
0.012

w5
-88.8855
0.0904
0

w6
149.7337
0
0

84

Scikit-learn Classes
ï‚– linear_model.Ridge :
ï‚– Linear Regression with L2 Regularization

ï‚– linear_model.Lasso :
ï‚– Linear Regression with L1 Regularization

85

L2, L1 Regularization: ë¬´ì—‡ì„ ì–¸ì œ ì‚¬ìš©í•˜ëŠ”ê°€?
ï‚– Overfittingì„ ë°©ì§€í•˜ê¸° ìœ„í•´ Regularization ì„ í•­ìƒ ì‚¬ìš©í•œë‹¤.
ï‚– ê¸°ë³¸ì ìœ¼ë¡œëŠ” L2ë¥¼ ì‚¬ìš©í•˜ë„ ë¬´ë°©í•¨.
ï‚– ìµœì†Œí•œì˜ ì£¼ìš” Featureì„ ì¶”ì¶œí•˜ê³  ì‹¶ì„ ê²½ìš° L1ì„ ì‚¬ìš©í•˜ë©´ ì¢‹ìŒ.

ï‚– Regularization Weight Î» ëŠ” ì–´ë–»ê²Œ ì •í•˜ëŠ”ê°€?
ï‚– Grid search
ï‚– ì—¬ëŸ¬ Î» ê°’ì„, Cross validationì„ ì‚¬ìš©í•˜ì—¬ í…ŒìŠ¤íŠ¸í•˜ì—¬ evaluation setì— ëŒ€í•œ ì„±ëŠ¥ì´ ê°€
ì¥ ë†’ì€ Î» ê°’ì„ ì‚¬ìš©.

86

Usage 2 :
Feature Importance analysis
87

Feature Importance Analysis
ï‚– ì—¬ëŸ¬ Featureë“¤ ì¤‘ì— ì–´ëŠ featureê°€ ê²°ë¡  (y)ë¥¼ ì˜ˆì¸¡í•˜ëŠ”ë° ì–¼ë§ˆë§Œí¼ ì¤‘ìš”í•œì§€
ë¶„ì„í•  ìˆ˜ ìˆë‹¤.
ì˜ˆ) í‰ê·  ì‹ì‚¬ëŸ‰, ìš´ë™ëŸ‰, ë³µë¶€ ë‘˜ë ˆ ì¤‘ ëª¸ë¬´ê²Œì— ê°€ì¥ ì˜í–¥ì„ ë¯¸ì¹˜ëŠ” featureëŠ”?

88

Wine Quality Data
Input Feature
1 - fixed acidity
2 - volatile acidity
3 - citric acid
4 - residual sugar
5 - chlorides
6 - free sulfur dioxide
7 - total sulfur dioxide
8 - density
9 - pH
10 - sulphates
11 â€“ alcohol

P. Cortez, A. Cerdeira, F. Almeida, T. Matos and J. Reis. Modeling
wine preferences by data mining from physicochemical properties.
In Decision Support Systems, Elsevier, 47(4):547-553, 2009.

https://archive.ics.uci.edu/ml/datasets/Wine+Quality

Output variable (based on sensory data):
12 - quality (score between 0 and 10)

89

Wine Quality â€“ Feature Weights
Wine Quality Feature Weights
5.00E+00
0.00E+00
-5.00E+00
-1.00E+01
-1.50E+01
-2.00E+01

Density featureì™€ ê°•í•œ Negative Relationì´ ê´€ì¸¡ëœë‹¤.

-2.50E+01

90

Wine Quality â€“ Feature value average and standard deviation

Feature Values
50
40
30
20
10
0

avg
sd

ë‹¤ë§Œ ì‹¤ì œ Density featureì˜ ê°’ í‰ê· ì„ ë³´ë©´, ì‹¤ì œ ê°’ì´ ë§¤ìš° ì‘ë‹¤.
Total sulfur dioxide ê°€ feature ê°’ í‰ê· ì´ ì œì¼ í°ë°, Total sulfur dioxideê°€ ë” ì¤‘ìš”í•˜ì§€ ì•Šì„ê¹Œ?

91

Normalization :
ì œëŒ€ë¡œ ëœ í•´ì„ì„ ìœ„í•´ì„œëŠ” ì„œë¡œ ë‹¤ë¥¸ Feature ì‚¬ì´ì˜ ê°’ì˜ ê·œëª¨(Scale)ì— ê³µí†µëœ ê¸°ì¤€ì´ í•„ìš”

ï‚– Feature ê°’ì˜ ê·œëª¨ê°€ ë‹¤ë¥¼ ê²½ìš° Feature Weightê°’ì˜ ë‹¨ìˆœë¹„êµëŠ” í˜ë“¤ë‹¤.

ï‚– ì˜ˆ ) ë‹¤ìŒê³¼ ê°™ì€ ê²½ìš°
ï‚– Density í‰ê· ì´ 0.5 ì´ê³  sdê°€ 0.1 ì—ì„œ Densityê°€ 0.2 ì˜¤ë¥´ë©´(0.7ì´ë˜ë©´) wine qualityê°€
1 ë‚´ë ¤ê°„ë‹¤.
ï‚– Total sulfur dioxide í‰ê· ì´ 50 ì´ê³  sdê°€ 10 ì—ì„œ Total sulfur dioxide ê°’ì´ 5 ì˜¤ë¥´ë©´(55
ê°€ ë˜ë©´) 1 ë‚´ë ¤ê°„ë‹¤.
ï‚– Question 1 : Densityì™€ Total sulfur dioxide ì¤‘ ì–´ëŠ featureì´ wine qualityì— ë” ë¶€ì •ì 
ì¸ ì˜í–¥ì„ ë¯¸ì¹˜ëŠ”ê°€?
ï‚– Question 2 : Linear Regressionì˜ weightì˜ ì ˆëŒ€ê°’ì€ Densityì˜ Weightì´ ë” í°ê°€? Total
sulfur dioxideì˜ weightê°€ ë” í°ê°€?

92

Normalization ì˜ˆ : Z-Score

Z-scoreê°€ ì˜ë¯¸í•˜ëŠ” ë‚´ìš©ì„ í’€ì–´ ì“°ë©´?

Image Source: https://www.simplypsychology.org/z-score.html

93

Wine Quality â€“ Feature Weights (Again)
Wine Quality Feature Weights
5.00E+00
0.00E+00
-5.00E+00
-1.00E+01
-1.50E+01
-2.00E+01
-2.50E+01

Wine Quality Feature Weights
(Z-Scored Feature)
0.4
0.3
0.2
0.1
0
-0.1
-0.2
-0.3

94

Feature Importance í•´ì„í•  ë•Œ ìœ ì˜ì : Correlation VS. Causation
ï‚– Causation : ì›ì¸-ê²°ê³¼ ê´€ê³„

ï‚– ì˜ˆ)
ï‚– ì›ì¸: ì§ì„ ê±°ë¦¬ 300 kmë¥¼ í‰ê· ì‹œì† 100km/hë¡œ ë‹¬ë ¸ë‹¤.
ï‚– ê²°ê³¼ : 3 ì‹œê°„ë§Œì— ì‹œì‘ì ì—ì„œ ëì ì— ë„ì°©í–ˆë‹¤.

ï‚– Correlation : ì—°ê´€ë˜ì–´ì„œ íŒ¨í„´ì´ ë‚˜íƒ€ë‚˜ì§€ë§Œ ì›ì¸-ê²°ê³¼ê´€ê³„ê°€ ìˆ
ëŠ”ì§€ ì—†ëŠ”ì§€ëŠ” ëª¨ë¥¸ë‹¤.
ï‚– ì˜ˆ) Drowning deaths and ice-cream sales are strongly correlated.

ï‚– Just because there's a strong correlation between two variables, there is
n't necessarily a causal relationship between them.
95

Correlation VS. Causation

https://www.decisionskills.com/blog/how-ice-cream-killsunderstanding-cause-and-effect

96

Summary
ï‚– Feature Importance Analysis : í•™ìŠµëœ Linear Regressionì˜ weight vectorë¥¼ ì‚¬ìš©
í•˜ì—¬ ì–´ë–¤ featureê°€ ì˜ˆì¸¡ ê°’ì— ì–´ëŠì •ë„ ì˜í–¥ì„ ì£¼ëŠ”ì§€ ë¶„ì„í•  ìˆ˜ ìˆë‹¤.
ï‚– Linear Regressionì„ ì‚¬ìš©í•œ Feature Importance Analysisë¥¼ ìˆ˜í–‰í•  ê²½ìš°, z-score
ë“±ì˜ ê¸°ë²•ì„ ì‚¬ìš©í•˜ì—¬ training/test dataì˜ ê°’ì„ normalizationí•œ ë‹¤ìŒ Modelì„
í•™ìŠµí•˜ì—¬ì•¼ í•œë‹¤.
ï‚– ë¶„ì„ì— ìˆì–´ì„œ featureì™€ ê²°ê³¼ (ì˜ˆì¸¡ ê°’)ì™€ì˜ ê´€ê³„ëŠ” Causation (ì›ì¸-ê²°ê³¼) ê´€
ê³„ê°€ ì•„ë‹Œ, Correlation (ì—°ê´€) ê´€ê³„ì„ì„ í•­ìƒ ìœ ì˜í•˜ë¼.

97

